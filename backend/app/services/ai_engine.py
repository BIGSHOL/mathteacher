"""AI Engine service using Google GenAI SDK with Pattern System Integration.

ì •í™•ë„ ë° ì‹ ë¢°ë„ í–¥ìƒ ì „ëµ:
1. ë™ì  í”„ë¡¬í”„íŠ¸ ë¹Œë” í†µí•©
2. ì‹œí—˜ì§€ ìœ í˜• ìë™ ë¶„ë¥˜ (ë¹ˆì‹œí—˜ì§€/í•™ìƒë‹µì•ˆ/ì±„ì ì—¬ë¶€)
3. ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„
4. íŒ¨í„´ ë§¤ì¹­ ì´ë ¥ ì¶”ì 
5. Chain of Thought í”„ë¡¬í”„íŒ…
6. ë¶„ì„ ê²°ê³¼ ìºì‹± (ì†ë„ ê°œì„ )
"""
import asyncio
import json
import logging
import time
from pathlib import Path
from typing import Any

# Google GenAI ë¡œê¹… ìˆ¨ê¸°ê¸° (AFC ë¡œê·¸ ë“±)
logging.getLogger("google_genai.models").setLevel(logging.WARNING)
logging.getLogger("google_genai").setLevel(logging.WARNING)

from google import genai
from google.genai import types
from fastapi import HTTPException, status

from app.core.config import settings
from app.db.supabase_client import SupabaseClient
from app.schemas.pattern import (
    ExamContext,
    BuildPromptRequest,
    ExamPaperClassification,
    QuestionAnswerInfo,
)
from app.services.analysis_cache import (
    get_analysis_cache,
    get_pattern_matcher,
    compute_file_hash,
    compute_analysis_cache_key,
)
from app.services.subject_config import (
    get_valid_question_types,
    get_valid_error_types,
)


class AIEngine:
    """Service for interacting with AI models with Pattern System Integration."""

    def __init__(self):
        self.api_key = settings.GEMINI_API_KEY
        self.model_name = settings.GEMINI_MODEL_NAME

        # Initialize client
        if self.api_key:
            self.client = genai.Client(api_key=self.api_key)
        else:
            self.client = None

        # ì €ì‹ ë¢°ë„ ì„ê³„ê°’ (ì´ ì´í•˜ë©´ null ì²˜ë¦¬)
        self.grading_confidence_threshold = 0.7

    # ============================================
    # 0. 2ë‹¨ê³„ ë¶„ì„: ì±„ì  í‘œì‹œ íƒì§€ (1ë‹¨ê³„)
    # ============================================
    async def detect_grading_marks(
        self,
        file_path: str,
    ) -> dict:
        """
        [1ë‹¨ê³„] ì±„ì  í‘œì‹œë§Œ ì§‘ì¤‘ íƒì§€

        Returns:
            {
                "marks": [
                    {
                        "question_number": 1,
                        "mark_type": "circle_on_answer",  # ë‹µì•ˆì— ë™ê·¸ë¼ë¯¸
                        "mark_symbol": "O",
                        "position": "on_student_answer",  # í•™ìƒ ë‹µì•ˆ ìœ„ì¹˜
                        "color": "red",  # red, blue, black, unknown
                        "indicates": "correct",  # correct, incorrect, uncertain
                        "confidence": 0.95
                    },
                    ...
                ],
                "overall_grading_status": "fully_graded",
                "color_distinction_possible": true,  # ìƒ‰ìƒ êµ¬ë¶„ ê°€ëŠ¥ ì—¬ë¶€
                "detection_notes": ["ë¹¨ê°„íœ í‘œì‹œ ê°ì§€", ...]
            }
        """
        if not self.client:
            return {"marks": [], "overall_grading_status": "unknown", "color_distinction_possible": False}

        # íŒŒì¼ ë¡œë“œ
        file_paths = [p.strip() for p in file_path.split(",")]
        file_parts = []

        for fp in file_paths:
            try:
                file_content, mime_type = await self._load_file_content(fp)
                if file_content:
                    file_parts.append(types.Part.from_bytes(data=file_content, mime_type=mime_type))
            except Exception as e:
                print(f"[Mark Detection] Error loading file {fp}: {e}")
                continue

        if not file_parts:
            return {"marks": [], "overall_grading_status": "unknown", "color_distinction_possible": False}

        # ì±„ì  í‘œì‹œ íƒì§€ ì „ìš© í”„ë¡¬í”„íŠ¸
        detection_prompt = """ë‹¹ì‹ ì€ ì‹œí—˜ì§€ ì±„ì  í‘œì‹œ íƒì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì¤‘ìš”: ëŒ€ë¶€ë¶„ í•™ìƒì´ ìê°€ì±„ì í•œ ì‹œí—˜ì§€ì…ë‹ˆë‹¤!
- ì„ ìƒë‹˜ ì±„ì : ê¹”ë”, ë¹¨ê°„íœ, ì¼ê´€ë¨
- **í•™ìƒ ì±„ì : ë„ˆì €ë¶„í•¨, íœ ìƒ‰ìƒ ì œê°ê°, ë¶ˆê·œì¹™**

í•™ìƒ ìê°€ì±„ì  íŠ¹ì§•:
- ì•„ë¬´ íœì´ë‚˜ ì‚¬ìš© (ê²€ì •, íŒŒë‘, ë¹¨ê°• í˜¼ìš©)
- ë™ê·¸ë¼ë¯¸/ì‚¬ì„ ì´ ëŒ€ì¶© ê·¸ë ¤ì§
- í‘œì‹œ í¬ê¸°ì™€ ìœ„ì¹˜ê°€ ë¶ˆê·œì¹™
- ì¼ê´€ì„± ì—†ì„ ìˆ˜ ìˆìŒ

**ë„ˆì €ë¶„í•´ë„ ì±„ì  í‘œì‹œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.**

## âš ï¸ í•µì‹¬ êµ¬ë¶„: ë¬¸í•­ë²ˆí˜¸ vs ë³´ê¸°ë²ˆí˜¸

```
ë¬¸í•­ë²ˆí˜¸: 1. 2. 3. 4. ... â† ì±„ì  í‘œì‹œ ìœ„ì¹˜!
ë³´ê¸°ë²ˆí˜¸: â‘ â‘¡â‘¢â‘£â‘¤       â† í•™ìƒì´ ë‹µ ê³ ë¥´ëŠ” ê³³ (ì±„ì  ì•„ë‹˜!)
```

### í•™ìƒì˜ ë‹µì•ˆ ì„ íƒ - ì±„ì  ì•„ë‹˜! ë¬´ì‹œ!
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ë‹¤ìŒ ì¤‘ ì˜³ì€ ê²ƒì€? (3ì )              â”‚
â”‚    â‘   â‘¡  â‘¢  â‘£  â‘¤                      â”‚
â”‚        â†‘                                â”‚
â”‚    ê²€ì •/íŒŒë‘íœìœ¼ë¡œ ë³´ê¸° â‘¡ì— ë™ê·¸ë¼ë¯¸     â”‚
â”‚    = í•™ìƒì´ ê³ ë¥¸ ë‹µ (ì±„ì  í‘œì‹œ ì•„ë‹˜!)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### ì±„ì ìì˜ ì±„ì  í‘œì‹œ - ì´ê²ƒë§Œ íƒì§€!
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—‹ 1. ë‹¤ìŒ ì¤‘ ì˜³ì€ ê²ƒì€? (3ì )           â”‚
â”‚ â†‘                                       â”‚
â”‚ ë¬¸í•­ë²ˆí˜¸ "1"ì— ë¹¨ê°„ ë™ê·¸ë¼ë¯¸ = ì •ë‹µ!     â”‚
â”‚    â‘   â‘¡  â‘¢  â‘£  â‘¤                      â”‚
â”‚                                         â”‚
â”‚ / 2. ë‹¤ìŒì„ ê³„ì‚°í•˜ì‹œì˜¤. (4ì )            â”‚
â”‚ â†‘                                       â”‚
â”‚ ë¬¸í•­ë²ˆí˜¸ "2"ì— ë¹¨ê°„ ì‚¬ì„  = ì˜¤ë‹µ!         â”‚
â”‚    â‘   â‘¡  â‘¢  â‘£  â‘¤                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ ì ˆëŒ€ í˜¼ë™ ê¸ˆì§€!
- ë³´ê¸°ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì— ë™ê·¸ë¼ë¯¸ â†’ í•™ìƒ ë‹µ ì„ íƒ (ë¬´ì‹œ!)
- ë¬¸í•­ë²ˆí˜¸(1. 2. 3.)ì— ë™ê·¸ë¼ë¯¸ â†’ ì±„ì  í‘œì‹œ (íƒì§€!)

## ì±„ì  í‘œì‹œ ìœ„ì¹˜ (ì‹¤ì œ íŒ¨í„´)

### ê°€ì¥ í”í•œ ìœ„ì¹˜
1. **ë¬¸í•­ë²ˆí˜¸ì—** - "1.", "2." ë“± ë²ˆí˜¸ ìœ„/ì˜†ì— O ë˜ëŠ” /
2. **ë¬¸í•­ ì „ì²´ì— í¬ê²Œ** - ë¬¸ì œ ì˜ì—­ ì „ì²´ë¥¼ ë®ëŠ” í° O ë˜ëŠ” /

### ë“œë¬¸ ìœ„ì¹˜ (ì°¸ê³ ìš©)
- í•™ìƒ ë‹µì•ˆ ë°”ë¡œ ì˜† (O, X)
- âŒ ë°°ì  ì˜†ì—ëŠ” ê±°ì˜ í‘œì‹œ ì•ˆ í•¨

## ì±„ì  í‘œì‹œ íŒì •

### ì •ë‹µ (indicates: "correct")
| í‘œì‹œ | ë¹ˆë„ | ì„¤ëª… |
|------|------|------|
| **ë™ê·¸ë¼ë¯¸ (O, â—‹)** | ê°€ì¥ ë§ìŒ | ë¬¸í•­ë²ˆí˜¸ì— ë™ê·¸ë¼ë¯¸ |

### ì˜¤ë‹µ (indicates: "incorrect")
| í‘œì‹œ | ë¹ˆë„ | ì„¤ëª… |
|------|------|------|
| **ì‚¬ì„  (/, \\, â•±)** | ê°€ì¥ ë§ìŒ | ë¬¸í•­ë²ˆí˜¸ë¥¼ ê¸‹ëŠ” ë¹—ê¸ˆ |
| X í‘œì‹œ (X, âœ—) | ìì£¼ ì‚¬ìš© | ë¬¸í•­ë²ˆí˜¸ì— X |
| ì²´í¬ (âœ“) | ì¼ë¶€ ì‚¬ìš© | ì„ ìƒë‹˜ë§ˆë‹¤ ë‹¤ë¦„ (ì˜¤ë‹µ ì˜ë¯¸ë¡œ ì“°ëŠ” ê²½ìš°) |
| ì •ë‹µ ê¸°ì¬ | ê°€ë” | í‹€ë¦° ë‹µ ì˜†ì— ì •ë‹µ ì¨ì¤Œ |

**ì°¸ê³ : ì²´í¬(âœ“)ëŠ” ì„ ìƒë‹˜ë§ˆë‹¤ ì •ë‹µ/ì˜¤ë‹µ ì˜ë¯¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ë‹¤ë¥¸ ë¬¸í•­ì˜ íŒ¨í„´ì„ ë³´ê³  íŒë‹¨.**

### ë¯¸ì±„ì  (indicates: "not_graded")
- ì±„ì  í‘œì‹œ ì—†ìŒ (íŠ¹íˆ ì„œìˆ í˜•ì€ ëŒ€ë¶€ë¶„ ë¯¸ì±„ì )
- í•™ìƒ ë‹µë§Œ ìˆê³  O// í‘œì‹œ ì—†ìŒ
- **ë¯¸ì±„ì  ë¬¸í•­ì€ ë°˜ë“œì‹œ not_gradedë¡œ í‘œì‹œ** (ì¶”ì¸¡ ê¸ˆì§€!)

### ë¶ˆí™•ì‹¤ (indicates: "uncertain")
- í‘ë°± ìŠ¤ìº”ìœ¼ë¡œ ìƒ‰ìƒ êµ¬ë¶„ ë¶ˆê°€
- í‘œì‹œê°€ íë¦¿í•¨

## âš ï¸ ì„œìˆ í˜• ë¬¸í•­ ì£¼ì˜
- ì„œìˆ í˜•ì€ ë‚˜ì¤‘ì— ë³„ë„ ì±„ì í•˜ë¯€ë¡œ **ëŒ€ë¶€ë¶„ ë¯¸ì±„ì  ìƒíƒœ**
- ì„œìˆ í˜•ì— ì±„ì  í‘œì‹œ ì—†ìœ¼ë©´ â†’ not_graded (ì¶”ì¸¡ ê¸ˆì§€!)
- ì ìˆ˜ê°€ ëª…í™•íˆ ê¸°ì¬ëœ ê²½ìš°ë§Œ ì±„ì ëœ ê²ƒìœ¼ë¡œ ì¸ì‹

## íŒë‹¨ ê¸°ì¤€: ìœ„ì¹˜ > ëª¨ì–‘ > ìƒ‰ìƒ

í•™ìƒ ìê°€ì±„ì ì€ ìƒ‰ìƒì´ ì œê°ê°ì´ë¯€ë¡œ **ìœ„ì¹˜ë¡œ íŒë‹¨**í•˜ì„¸ìš”.

| ìœ„ì¹˜ | í‘œì‹œ | ì˜ë¯¸ |
|------|------|------|
| **ë¬¸í•­ë²ˆí˜¸(1. 2. 3.)ì—** | O, ë™ê·¸ë¼ë¯¸ | âœ… ì •ë‹µ |
| **ë¬¸í•­ë²ˆí˜¸(1. 2. 3.)ì—** | /, ì‚¬ì„ , X | âœ… ì˜¤ë‹µ |
| ë³´ê¸°ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì— | ë™ê·¸ë¼ë¯¸ | âŒ í•™ìƒ ë‹µ ì„ íƒ (ë¬´ì‹œ) |

### ìƒ‰ìƒì€ ì°¸ê³ ë§Œ
- í•™ìƒ ìê°€ì±„ì : ì•„ë¬´ íœì´ë‚˜ ì‚¬ìš©
- ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„í•˜ì§€ ë§ê³  **ìœ„ì¹˜ì™€ ëª¨ì–‘**ìœ¼ë¡œ íŒë‹¨

## JSON ì‘ë‹µ

```json
{
    "marks": [
        {
            "question_number": 1,
            "mark_type": "circle",
            "mark_symbol": "â—‹",
            "position": "on_question_number",
            "color": "red",
            "indicates": "correct",
            "confidence": 0.95
        },
        {
            "question_number": 2,
            "mark_type": "slash",
            "mark_symbol": "/",
            "position": "on_question_number",
            "color": "red",
            "indicates": "incorrect",
            "confidence": 0.92
        },
        {
            "question_number": 3,
            "mark_type": "none",
            "indicates": "not_graded",
            "confidence": 0.85
        }
    ],
    "overall_grading_status": "partially_graded",
    "color_distinction_possible": true
}
```

## ì‹ ë¢°ë„ ê¸°ì¤€ (í•™ìƒ ìê°€ì±„ì  ê³ ë ¤)
- ë¬¸í•­ë²ˆí˜¸ì— O// í‘œì‹œ (ê¹”ë”í•˜ë“  ë„ˆì €ë¶„í•˜ë“ ) â†’ 0.85+
- ë¬¸í•­ ì „ì²´ì— í° O// í‘œì‹œ â†’ 0.8+
- ëŒ€ì¶© ê·¸ë¦° ë™ê·¸ë¼ë¯¸/ì‚¬ì„ ë„ ì¸ì‹ â†’ 0.75+
- ë³´ê¸°ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì˜ ë™ê·¸ë¼ë¯¸ â†’ 0.3 ì´í•˜ (í•™ìƒ ë‹µ ì„ íƒ)

ëª¨ë“  ë¬¸í•­ì— ëŒ€í•´ **ë¹¨ê°„íœ ì±„ì  í‘œì‹œë§Œ** íƒì§€í•˜ì„¸ìš”.
"""

        try:
            all_parts = file_parts + [types.Part.from_text(text=detection_prompt)]

            # ì±„ì  í‘œì‹œ íƒì§€ (2ë¶„ íƒ€ì„ì•„ì›ƒ)
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        self.client.models.generate_content,
                        model=self.model_name,
                        contents=[types.Content(role="user", parts=all_parts)],
                        config=types.GenerateContentConfig(
                            response_mime_type="application/json",
                            temperature=0.1,
                            max_output_tokens=8192,
                        ),
                    ),
                    timeout=120.0  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                )
            except asyncio.TimeoutError:
                print("[Mark Detection] íƒ€ì„ì•„ì›ƒ - ì±„ì  í‘œì‹œ íƒì§€ ê±´ë„ˆëœ€")
                return {"marks": [], "overall_grading_status": "unknown", "color_distinction_possible": False}

            if not response.text:
                return {"marks": [], "overall_grading_status": "unknown", "color_distinction_possible": False}

            result = self._parse_json_response(response.text)
            print(f"[Mark Detection] Detected {len(result.get('marks', []))} marks")
            return result

        except Exception as e:
            print(f"[Mark Detection Error] {e}")
            return {"marks": [], "overall_grading_status": "unknown", "color_distinction_possible": False, "error": str(e)}

    def _build_grading_context_from_marks(self, marks_result: dict) -> str:
        """íƒì§€ëœ ì±„ì  í‘œì‹œë¥¼ ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•  ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        marks = marks_result.get("marks", [])
        if not marks:
            return ""

        lines = ["\n\n## ğŸ” [1ë‹¨ê³„] ì±„ì  í‘œì‹œ íƒì§€ ê²°ê³¼ (ì°¸ê³ ìš©)\n"]
        lines.append("ì•„ë˜ëŠ” ë³„ë„ ë¶„ì„ì—ì„œ íƒì§€ëœ ì±„ì  í‘œì‹œì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ **ì°¸ê³ í•˜ì—¬** ì •ì˜¤ë‹µì„ íŒì •í•˜ì„¸ìš”.\n")
        lines.append("ë‹¨, íƒì§€ ê²°ê³¼ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ ì§ì ‘ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìµœì¢… íŒë‹¨í•˜ì„¸ìš”.\n\n")

        lines.append("| ë¬¸í•­ | í‘œì‹œ | ìœ„ì¹˜ | ìƒ‰ìƒ | íŒì • | ì‹ ë¢°ë„ |\n")
        lines.append("|------|------|------|------|------|--------|\n")

        for m in marks:
            q_num = m.get("question_number", "?")
            symbol = m.get("mark_symbol") or "-"
            position = m.get("position") or "-"
            color = m.get("color") or "-"
            indicates = m.get("indicates", "uncertain")
            conf = m.get("confidence", 0)

            # íŒì • í•œê¸€í™”
            indicates_kr = {
                "correct": "âœ…ì •ë‹µ",
                "incorrect": "âŒì˜¤ë‹µ",
                "not_graded": "â¬œë¯¸ì±„ì ",
                "uncertain": "â“ë¶ˆí™•ì‹¤"
            }.get(indicates, indicates)

            lines.append(f"| {q_num} | {symbol} | {position} | {color} | {indicates_kr} | {conf:.0%} |\n")

        # ìƒ‰ìƒ êµ¬ë¶„ ê°€ëŠ¥ ì—¬ë¶€
        if marks_result.get("color_distinction_possible"):
            lines.append("\nâœ… ì´ ì´ë¯¸ì§€ì—ì„œ ìƒ‰ìƒ êµ¬ë¶„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n")
        else:
            lines.append("\nâš ï¸ ì´ ì´ë¯¸ì§€ì—ì„œ ìƒ‰ìƒ êµ¬ë¶„ì´ ì–´ë µìŠµë‹ˆë‹¤. í‘œì‹œ ìœ„ì¹˜ì™€ ëª¨ì–‘ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.\n")

        # íƒì§€ ë…¸íŠ¸
        notes = marks_result.get("detection_notes", [])
        if notes:
            lines.append("\níƒì§€ ë…¸íŠ¸:\n")
            for note in notes:
                lines.append(f"- {note}\n")

        lines.append("\n**ìœ„ íƒì§€ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ë˜, ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë³´ê³  ìµœì¢… íŒë‹¨í•˜ì„¸ìš”.**\n")
        lines.append("**íƒì§€ ì‹ ë¢°ë„ê°€ 70% ë¯¸ë§Œì´ë©´ ì§ì ‘ í™•ì¸ í›„ íŒì •í•˜ì„¸ìš”.**\n")

        return "".join(lines)

    def _cross_validate_grading(self, analysis_result: dict, marks_result: dict) -> dict:
        """
        [êµì°¨ ê²€ì¦] 1ë‹¨ê³„ íƒì§€ ê²°ê³¼ì™€ 2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ë¶ˆì¼ì¹˜ í•´ì†Œ

        ê·œì¹™:
        1. ë‘ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ë©´ ì‹ ë¢°ë„ ìƒìŠ¹
        2. ë¶ˆì¼ì¹˜ + íƒì§€ ì‹ ë¢°ë„ ë†’ìŒ â†’ íƒì§€ ê²°ê³¼ë¡œ ìˆ˜ì •
        3. ë¶ˆì¼ì¹˜ + íƒì§€ ì‹ ë¢°ë„ ë‚®ìŒ â†’ ë¶„ì„ ê²°ê³¼ ìœ ì§€ (ë¶ˆí™•ì‹¤ í”Œë˜ê·¸)
        4. íƒì§€ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¯¸ì±„ì ì´ë©´ ë¶„ì„ ê²°ê³¼ ìœ ì§€
        5. ì €ì‹ ë¢°ë„(< 0.7)ë©´ nullë¡œ ë³€ê²½ (ì¶”ì¸¡ ë°©ì§€)
        """
        marks = marks_result.get("marks", [])
        if not marks:
            return analysis_result

        questions = analysis_result.get("questions", [])
        if not questions:
            return analysis_result

        # íƒì§€ ê²°ê³¼ë¥¼ ë¬¸í•­ë²ˆí˜¸ë¡œ ì¸ë±ì‹±
        marks_by_num = {}
        for m in marks:
            q_num = m.get("question_number")
            if q_num:
                marks_by_num[q_num] = m

        corrections_made = 0
        confidence_boosts = 0
        null_conversions = 0

        for q in questions:
            q_num = q.get("question_number")
            if not q_num or q_num not in marks_by_num:
                continue

            mark = marks_by_num[q_num]
            mark_indicates = mark.get("indicates")
            mark_confidence = mark.get("confidence", 0)
            analysis_is_correct = q.get("is_correct")

            # íƒì§€ ê²°ê³¼ ë³€í™˜
            if mark_indicates == "correct":
                mark_is_correct = True
            elif mark_indicates == "incorrect":
                mark_is_correct = False
            elif mark_indicates == "not_graded":
                mark_is_correct = None
            else:  # uncertain
                mark_is_correct = None

            # ì €ì‹ ë¢°ë„ íƒì§€ â†’ nullë¡œ ë³€í™˜ (ì¶”ì¸¡ ë°©ì§€)
            if mark_confidence < self.grading_confidence_threshold and mark_indicates not in ["not_graded"]:
                # ë¶„ì„ ê²°ê³¼ë„ ì €ì‹ ë¢°ë„ë©´ nullë¡œ
                q_confidence = q.get("confidence", 0.5)
                if q_confidence < self.grading_confidence_threshold:
                    if q.get("is_correct") is not None:
                        q["is_correct"] = None
                        q["earned_points"] = None
                        q["_grading_note"] = f"ì €ì‹ ë¢°ë„ë¡œ ë¯¸ì±„ì  ì²˜ë¦¬ (íƒì§€:{mark_confidence:.0%}, ë¶„ì„:{q_confidence:.0%})"
                        null_conversions += 1
                continue

            # ë‘ ê²°ê³¼ ë¹„êµ
            if analysis_is_correct == mark_is_correct:
                # ì¼ì¹˜ â†’ ì‹ ë¢°ë„ ìƒìŠ¹
                current_conf = q.get("confidence", 0.5)
                q["confidence"] = min(1.0, current_conf + 0.1)
                q["_grading_validated"] = True
                confidence_boosts += 1

            elif mark_confidence >= 0.85 and mark_is_correct is not None:
                # ë¶ˆì¼ì¹˜ + íƒì§€ ê³ ì‹ ë¢°ë„ â†’ íƒì§€ ê²°ê³¼ë¡œ ìˆ˜ì •
                old_value = q.get("is_correct")
                q["is_correct"] = mark_is_correct
                q["_grading_corrected"] = True
                q["_grading_note"] = f"íƒì§€ ê²°ê³¼ë¡œ ìˆ˜ì • (ê¸°ì¡´: {old_value}, íƒì§€ ì‹ ë¢°ë„: {mark_confidence:.0%})"

                # íšë“ ì ìˆ˜ ì¬ê³„ì‚°
                if mark_is_correct is True:
                    q["earned_points"] = q.get("points", 0)
                elif mark_is_correct is False:
                    q["earned_points"] = 0
                else:
                    q["earned_points"] = None

                corrections_made += 1
                print(f"[Cross-Validate] Q{q_num}: {old_value} â†’ {mark_is_correct} (íƒì§€ ì‹ ë¢°ë„: {mark_confidence:.0%})")

            elif mark_is_correct is None and analysis_is_correct is not None:
                # íƒì§€=ë¯¸ì±„ì , ë¶„ì„=ì±„ì ë¨ â†’ ë¶„ì„ì´ ì¶”ì¸¡í–ˆì„ ê°€ëŠ¥ì„±
                # ë¶„ì„ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ nullë¡œ ë³€ê²½
                q_confidence = q.get("confidence", 0.5)
                if q_confidence < 0.8:
                    old_value = q.get("is_correct")
                    q["is_correct"] = None
                    q["earned_points"] = None
                    q["_grading_note"] = f"íƒì§€ì—ì„œ ë¯¸ì±„ì ìœ¼ë¡œ ê°ì§€ë¨, ë¶„ì„ ì¶”ì¸¡ ì œê±° (ê¸°ì¡´: {old_value})"
                    null_conversions += 1
                    print(f"[Cross-Validate] Q{q_num}: {old_value} â†’ null (ë¯¸ì±„ì  ê°ì§€)")

        # êµì°¨ ê²€ì¦ ê²°ê³¼ ê¸°ë¡
        analysis_result["_cross_validation"] = {
            "marks_detected": len(marks),
            "corrections_made": corrections_made,
            "confidence_boosts": confidence_boosts,
            "null_conversions": null_conversions,
        }

        if corrections_made > 0 or null_conversions > 0:
            print(f"[Cross-Validate] ì™„ë£Œ: {corrections_made}ê°œ ìˆ˜ì •, {confidence_boosts}ê°œ ì‹ ë¢°ë„ ìƒìŠ¹, {null_conversions}ê°œ null ë³€í™˜")

        return analysis_result

    def _consolidate_dominant_topic(self, analysis_result: dict, threshold: float = 0.6) -> dict:
        """
        [ê³¼ëª© í†µí•©] ì••ë„ì  ë¹„ìœ¨ì˜ ê³¼ëª©ìœ¼ë¡œ ì „ì²´ ë¬¸í•­ í†µí•©

        ë‚´ì‹  ì‹œí—˜ì§€ëŠ” í•˜ë‚˜ì˜ êµì¬ì—ì„œ ì¶œì œë˜ë¯€ë¡œ, íŠ¹ì • ê³¼ëª©ì´ 60% ì´ìƒì´ë©´
        ë‚˜ë¨¸ì§€ ë¬¸í•­ë„ ê°™ì€ ê³¼ëª©ìœ¼ë¡œ ë¶„ë¥˜

        Args:
            analysis_result: ë¶„ì„ ê²°ê³¼
            threshold: í†µí•© ê¸°ì¤€ ë¹„ìœ¨ (ê¸°ë³¸ê°’ 0.6 = 60%)

        Returns:
            ê³¼ëª©ì´ í†µí•©ëœ ë¶„ì„ ê²°ê³¼
        """
        questions = analysis_result.get("questions", [])
        if len(questions) < 3:  # ë¬¸í•­ì´ ë„ˆë¬´ ì ìœ¼ë©´ í†µí•© ì•ˆí•¨
            return analysis_result

        # 1. ê³¼ëª©ë³„ ë¬¸í•­ ìˆ˜ ê³„ì‚°
        subject_counts = {}
        for q in questions:
            topic = q.get("topic", "")
            if not topic or " > " not in topic:
                continue

            # topic í˜•ì‹: "ê³¼ëª©ëª… > ëŒ€ë‹¨ì› > ì†Œë‹¨ì›"
            subject = topic.split(" > ")[0].strip()
            subject_counts[subject] = subject_counts.get(subject, 0) + 1

        if not subject_counts:
            return analysis_result

        # 2. ê°€ì¥ ë§ì€ ê³¼ëª© ì°¾ê¸°
        total_questions = len(questions)
        dominant_subject = max(subject_counts, key=subject_counts.get)
        dominant_count = subject_counts[dominant_subject]
        dominant_ratio = dominant_count / total_questions

        # 3. 60% ì´ìƒì´ë©´ ì „ì²´ í†µí•©
        if dominant_ratio >= threshold:
            print(f"[Topic Consolidation] {dominant_subject}: {dominant_count}/{total_questions} ({dominant_ratio:.0%}) - ì „ì²´ í†µí•© ì ìš©")

            # ì£¼ìš” ê³¼ëª©ì˜ ëŒ€ë‹¨ì›ë³„ ë¬¸í•­ ìˆ˜ íŒŒì•… (ì¬ë¶„ë¥˜ ì‹œ ì°¸ê³ )
            chapter_examples = {}
            for q in questions:
                topic = q.get("topic", "")
                if topic.startswith(dominant_subject + " > "):
                    parts = topic.split(" > ")
                    if len(parts) >= 2:
                        chapter = parts[1].strip()
                        if chapter not in chapter_examples:
                            chapter_examples[chapter] = []
                        chapter_examples[chapter].append(topic)

            # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ëŒ€ë‹¨ì› ì°¾ê¸° (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
            default_chapter = max(chapter_examples, key=lambda k: len(chapter_examples[k])) if chapter_examples else "ê¸°íƒ€"
            default_topic = chapter_examples[default_chapter][0] if chapter_examples.get(default_chapter) else f"{dominant_subject} > {default_chapter} > ê¸°íƒ€"

            # ëª¨ë“  ë¬¸í•­ì„ ì£¼ìš” ê³¼ëª©ìœ¼ë¡œ ì¬ë¶„ë¥˜
            consolidated_count = 0
            for q in questions:
                topic = q.get("topic", "")
                if not topic or not topic.startswith(dominant_subject + " > "):
                    # ë‹¤ë¥¸ ê³¼ëª©ì´ê±°ë‚˜ topicì´ ì—†ëŠ” ê²½ìš° â†’ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                    old_topic = topic
                    q["topic"] = default_topic
                    q["_topic_consolidated"] = True
                    q["_original_topic"] = old_topic
                    consolidated_count += 1

            if consolidated_count > 0:
                print(f"[Topic Consolidation] {consolidated_count}ê°œ ë¬¸í•­ì„ '{dominant_subject}'ë¡œ í†µí•©")
                analysis_result["_topic_consolidation"] = {
                    "dominant_subject": dominant_subject,
                    "dominant_count": dominant_count,
                    "dominant_ratio": round(dominant_ratio, 2),
                    "consolidated_count": consolidated_count,
                    "threshold": threshold,
                }

        return analysis_result

    # ============================================
    # 1. ì‹œí—˜ì§€ ìœ í˜• ìë™ ë¶„ë¥˜
    # ============================================
    async def classify_exam_paper(
        self,
        file_path: str,
    ) -> ExamPaperClassification:
        """ì‹œí—˜ì§€ ìœ í˜• ìë™ ë¶„ë¥˜ (ë¹ˆì‹œí—˜ì§€/í•™ìƒë‹µì•ˆ/ì±„ì ìƒíƒœ)"""
        if not self.client:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ExamPaperClassification(
                paper_type="unknown",
                confidence=0.0,
                indicators=["AI ì„œë¹„ìŠ¤ ë¯¸ì„¤ì •"],
                grading_status="unknown",
            )

        # íŒŒì¼ ë¡œë“œ
        file_paths = [p.strip() for p in file_path.split(",")]
        file_parts = []

        for fp in file_paths:
            try:
                file_content, mime_type = await self._load_file_content(fp)
                if file_content:
                    file_parts.append(types.Part.from_bytes(data=file_content, mime_type=mime_type))
            except Exception as e:
                print(f"[Classification] Error loading file {fp}: {e}")
                continue

        if not file_parts:
            return ExamPaperClassification(
                paper_type="unknown",
                confidence=0.0,
                indicators=["íŒŒì¼ ì—†ìŒ"],
                grading_status="unknown",
            )

        # ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
        classification_prompt = """ì´ ì‹œí—˜ì§€ ì´ë¯¸ì§€ë¥¼ **ì„¸ì‹¬í•˜ê²Œ** ë¶„ì„í•˜ì—¬ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

## ë¶„ë¥˜ í•­ëª©

### 1. paper_type (ì‹œí—˜ì§€ ìœ í˜•) - âš ï¸ í•µì‹¬ íŒì •

ğŸ” **ë¬¸í•­ ë²ˆí˜¸ vs ë³´ê¸° ë²ˆí˜¸ êµ¬ë¶„!**

```
ë¬¸í•­ ë²ˆí˜¸: 1. 2. 3. (ë¬¸ì œ ì¢Œìƒë‹¨, ì•„ë¼ë¹„ì•„ ìˆ«ì)
ë³´ê¸° ë²ˆí˜¸: â‘  â‘¡ â‘¢ â‘£ â‘¤ (ê°ê´€ì‹ ì„ íƒì§€, ì›ë¬¸ì)
```

**"answered"ë¡œ íŒì • (í•˜ë‚˜ë¼ë„ ë³´ì´ë©´ answered!):**
- âœ… **ë¬¸í•­ ë²ˆí˜¸(1. 2. 3.)ì— ë™ê·¸ë¼ë¯¸** â†’ ì •ë‹µ í‘œì‹œ = ì±„ì ë¨!
- âŒ **ë¬¸í•­ ë²ˆí˜¸ì— Xí‘œì‹œ, ë¹—ê¸ˆ(/), ì‚¬ì„ ** â†’ ì˜¤ë‹µ í‘œì‹œ = ì±„ì ë¨!
- ğŸ”´ **O, â—‹, âœ“ í‘œì‹œ** â†’ ì •ë‹µ í‘œì‹œ = ì±„ì ë¨!
- ğŸ”´ **ì ìˆ˜ ê¸°ì¬** (3ì , 0ì , 5/9 ë“±) â†’ ì±„ì ë¨!
- ğŸ“ ë³´ê¸° ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì— ì²´í¬/ë™ê·¸ë¼ë¯¸ â†’ í•™ìƒ ë‹µì•ˆ = answered!
- ğŸ“ ì„œìˆ í˜•ì— ì†ê¸€ì”¨ í’€ì´
- ğŸ“ ê³„ì‚° í”ì /ë©”ëª¨

**"blank"ë¡œ íŒì •:**
- ë¬¸í•­ ë²ˆí˜¸ì— **ì•„ë¬´ í‘œì‹œë„ ì—†ìŒ**
- ë³´ê¸°ì— ì²´í¬ **ì—†ìŒ**
- ì†ê¸€ì”¨ **ì „í˜€ ì—†ìŒ**

âš ï¸ **í•µì‹¬: ë¬¸í•­ ë²ˆí˜¸(1. 2. 3.)ë‚˜ ë³´ê¸°(â‘ â‘¡â‘¢)ì— í‘œì‹œê°€ ìˆìœ¼ë©´ "answered"!**

### 2. grading_status (ì±„ì  ìƒíƒœ) - ë§¤ìš° ì¤‘ìš”!
- "not_graded": O/X í‘œì‹œê°€ **ì „í˜€** ì—†ìŒ
- "partially_graded": ì¼ë¶€ ë¬¸í•­ì—ë§Œ O/X í‘œì‹œ
- "fully_graded": ëŒ€ë¶€ë¶„ ë¬¸í•­ì— O/X í‘œì‹œ

## âš ï¸ ì±„ì  í‘œì‹œ íŒë‹¨ ê¸°ì¤€ (í•µì‹¬!)

### ì±„ì ë¨ (grading_status â‰  "not_graded")ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²½ìš°:
- ë¬¸í•­ì— O, â—‹, âœ“, ì²´í¬ í‘œì‹œ ì¡´ì¬
- ë¬¸í•­ì— X, âœ—, ë¹—ê¸ˆ(/) í‘œì‹œ ì¡´ì¬
- ì ìˆ˜ê°€ ê¸°ì¬ë˜ì–´ ìˆìŒ (3ì , 0ì  ë“±)
- ë¹¨ê°„íœìœ¼ë¡œ ì •ë‹µì„ ë”°ë¡œ ì¨ì¤Œ
- **ë¬¸ì œë²ˆí˜¸ì— ë™ê·¸ë¼ë¯¸** â†’ ì •ë‹µ í‘œì‹œ = ì±„ì ë¨!

### ë¯¸ì±„ì  (grading_status = "not_graded")ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²½ìš°:
- í•™ìƒ ë‹µë§Œ ìˆê³  O/X í‘œì‹œê°€ **ì „í˜€ ì—†ìŒ**
- ì ìˆ˜ ê¸°ì¬ ì—†ìŒ
- ì±„ì ìì˜ íœ í”ì  ì—†ìŒ

### ì •ì˜¤ë‹µ íŒì • í…Œì´ë¸”

| í‘œì‹œ | ìœ„ì¹˜ | ì˜ë¯¸ | grading_result |
|------|------|------|----------------|
| O, â—‹, âœ“ | í•™ìƒ ë‹µì•ˆ ì˜† | ì •ë‹µ | "correct" |
| X, âœ—, / | í•™ìƒ ë‹µì•ˆ ì˜† | ì˜¤ë‹µ | "incorrect" |
| ë™ê·¸ë¼ë¯¸ | **ë¬¸ì œë²ˆí˜¸** ì˜† | ì •ë‹µ í‘œì‹œ | "correct" |
| ë¹¨ê°„íœ ì •ë‹µ | ë¬¸í•­ ê·¼ì²˜ | í•™ìƒ ë‹µì´ í‹€ë¦¼ | "incorrect" |
| ì—†ìŒ | - | ë¯¸ì±„ì  | null |

## ì‘ë‹µ í˜•ì‹ (JSON)
{
    "paper_type": "answered",
    "paper_type_confidence": 0.95,
    "paper_type_indicators": ["ì†ê¸€ì”¨ ë‹µì•ˆ ê°ì§€", "ì—¬ëŸ¬ ë¬¸í•­ì— ë‹µì•ˆ ì‘ì„±"],
    "grading_status": "fully_graded",
    "grading_confidence": 0.90,
    "grading_indicators": ["O/X í‘œì‹œ ë°œê²¬", "ì ìˆ˜ ê¸°ì¬ í™•ì¸"],
    "total_questions": 10,
    "question_details": [
        {
            "question_number": 1,
            "has_answer": true,
            "has_grading_mark": true,
            "grading_result": "correct",
            "confidence": 0.95
        },
        {
            "question_number": 2,
            "has_answer": true,
            "has_grading_mark": true,
            "grading_result": "incorrect",
            "confidence": 0.90,
            "note": "ë¬¸ì œë²ˆí˜¸ì— Xí‘œì‹œ = í‹€ë¦° ë¬¸ì œ í‘œì‹œ"
        },
        {
            "question_number": 3,
            "has_answer": true,
            "has_grading_mark": false,
            "grading_result": null,
            "confidence": 0.85,
            "note": "O/X í‘œì‹œ ì—†ìŒ - ë¯¸ì±„ì "
        }
    ],
    "summary": {
        "answered_count": 10,
        "correct_count": 7,
        "incorrect_count": 2,
        "ungraded_count": 1,
        "blank_count": 0
    }
}
"""

        try:
            all_parts = file_parts + [types.Part.from_text(text=classification_prompt)]

            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[
                    types.Content(role="user", parts=all_parts),
                ],
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.1,
                    max_output_tokens=4096,
                ),
            )

            if not response.text:
                raise ValueError("Empty response")

            result = self._parse_json_response(response.text)

            # ExamPaperClassification ê°ì²´ë¡œ ë³€í™˜
            question_details = []
            for q in result.get("question_details", []):
                grading_result = q.get("grading_result")
                if grading_result == "correct":
                    answer_status = "correct"
                elif grading_result == "incorrect":
                    answer_status = "incorrect"
                elif not q.get("has_answer"):
                    answer_status = "blank"
                else:
                    answer_status = "unknown"

                question_details.append(QuestionAnswerInfo(
                    question_number=q.get("question_number", 0),
                    answer_status=answer_status,
                    has_grading_mark=q.get("has_grading_mark", False),
                    grading_result=answer_status if q.get("has_grading_mark") else None,
                    confidence=q.get("confidence", 0.5),
                ))

            summary = result.get("summary") or {}

            return ExamPaperClassification(
                paper_type=result.get("paper_type", "unknown"),
                confidence=result.get("paper_type_confidence") or 0.5,
                indicators=result.get("paper_type_indicators") or [],
                grading_status=result.get("grading_status", "unknown"),
                grading_indicators=result.get("grading_indicators") or [],
                question_details=question_details,
                total_questions=result.get("total_questions") or 0,
                answered_count=summary.get("answered_count") or 0,
                correct_count=summary.get("correct_count") or 0,
                incorrect_count=summary.get("incorrect_count") or 0,
                blank_count=summary.get("blank_count") or 0,
            )

        except Exception as e:
            print(f"[Classification Error] {e}")
            return ExamPaperClassification(
                paper_type="unknown",
                confidence=0.0,
                indicators=[f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}"],
                grading_status="unknown",
            )

    # ============================================
    # 2. ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
    # ============================================
    async def build_dynamic_prompt(
        self,
        db: SupabaseClient,
        exam_context: ExamContext,
        include_error_patterns: bool = True,
        include_examples: bool = True,
    ) -> str:
        """íŒ¨í„´ DB ê¸°ë°˜ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        from app.services.prompt_builder import PromptBuilder

        try:
            builder = PromptBuilder(db)
            request = BuildPromptRequest(
                exam_context=exam_context,
                include_error_patterns=include_error_patterns,
                include_examples=include_examples,
                max_examples_per_pattern=2,
            )
            result = await builder.build(request)
            return result.combined_prompt
        except Exception as e:
            print(f"[Dynamic Prompt Error] {e}")
            # í´ë°±: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            if exam_context.exam_paper_type == "blank":
                return self._get_blank_prompt()
            else:
                return self._get_student_prompt()

    # ============================================
    # 3. í†µí•© ë¶„ì„ (íŒ¨í„´ ì‹œìŠ¤í…œ í¬í•¨) - ë¶„ë¥˜ í†µí•© ë²„ì „
    # ============================================
    async def analyze_exam_with_patterns(
        self,
        db: SupabaseClient,
        file_path: str,
        grade_level: str | None = None,
        unit: str | None = None,
        category: str | None = None,
        exam_scope: list[str] | None = None,
        auto_classify: bool = True,
        exam_id: str | None = None,
        analysis_mode: str = "full",
        user_id: str | None = None,
        subject: str = "ìˆ˜í•™",
    ) -> dict:
        """
        íŒ¨í„´ ì‹œìŠ¤í…œì„ í™œìš©í•œ í†µí•© ë¶„ì„ (ë¶„ë¥˜ í†µí•© - ë‹¨ì¼ API í˜¸ì¶œ)

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš° ì½¤ë§ˆë¡œ êµ¬ë¶„)
            grade_level: í•™ë…„ (ì˜ˆ: "ì¤‘1", "ê³ 1")
            unit: ë‹¨ì› (ì˜ˆ: "ì´ì°¨ë°©ì •ì‹")
            category: ì„¸ë¶€ ê³¼ëª© (ì˜ˆ: "ê³µí†µìˆ˜í•™1", "ê³µí†µìˆ˜í•™2")
            auto_classify: ì‹œí—˜ì§€ ìœ í˜• ìë™ ë¶„ë¥˜ ì—¬ë¶€ (í†µí•© ë²„ì „ì—ì„œëŠ” í•­ìƒ ë¶„ì„ ë‚´ì—ì„œ ìˆ˜í–‰)
            exam_id: ì‹œí—˜ì§€ ID (ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ìš©)
            analysis_mode: ë¶„ì„ ëª¨ë“œ (questions_only: ë¬¸í•­ë§Œ, full: ì „ì²´, answers_only: ì •ì˜¤ë‹µë§Œ)
            user_id: ì‚¬ìš©ì ID (ë¶„ì„ ë¡œê¹…ìš©)
            subject: ê³¼ëª© (ìˆ˜í•™/ì˜ì–´)

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()

        # Analytics ë¡œê¹…: ë¶„ì„ ì‹œì‘
        if user_id and exam_id:
            try:
                from app.services.analytics_log import get_analytics_log_service
                analytics = get_analytics_log_service(db)
                await analytics.log_analysis_start(
                    user_id=user_id,
                    exam_id=exam_id,
                    metadata={
                        "analysis_mode": analysis_mode,
                        "grade_level": grade_level,
                        "unit": unit,
                    }
                )
            except Exception as e:
                print(f"[Analytics Log Error] {e}")

        # í—¬í¼: ë¶„ì„ ë‹¨ê³„ ì—…ë°ì´íŠ¸
        async def update_step(step: int):
            if exam_id and db:
                try:
                    await db.table("exams").eq("id", exam_id).update({"analysis_step": step}).execute()
                except Exception as e:
                    print(f"[Step Update Error] {e}")

        # ============ ìºì‹± ì‹œìŠ¤í…œ ============
        cache = get_analysis_cache()
        file_hash = None

        # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìºì‹œ í‚¤ ìƒì„±ìš©)
        try:
            file_paths = [p.strip() for p in file_path.split(",")]
            combined_content = b""
            for fp in file_paths:
                content, _ = await self._load_file_content(fp)
                if content:
                    combined_content += content
            if combined_content:
                file_hash = compute_file_hash(combined_content)
                cache_key = compute_analysis_cache_key(file_hash, grade_level, unit)

                # ìºì‹œ íˆíŠ¸ í™•ì¸
                cached_result = cache.get(cache_key)
                if cached_result:
                    elapsed = time.time() - start_time
                    print(f"[Cache HIT] {cache_key[:20]}... ({elapsed:.2f}ì´ˆ)")
                    cached_result["_cache_hit"] = True
                    cached_result["_elapsed_seconds"] = elapsed
                    return cached_result
        except Exception as e:
            print(f"[Cache Error] {e}")

        # 1. ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¶„ì„ ëª¨ë“œì— ë”°ë¼ ì„ íƒ)
        await update_step(1)
        is_questions_only = analysis_mode == "questions_only"
        print(f"[Step 1] í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘... (mode={analysis_mode})")

        exam_context = ExamContext(
            grade_level=grade_level,
            subject=subject,
            unit=unit,
            category=category,  # ì„¸ë¶€ ê³¼ëª© (ê³µí†µìˆ˜í•™1, ê³µí†µìˆ˜í•™2 ë“±)
            exam_scope=exam_scope,  # ì¶œì œë²”ìœ„ (ë‹¨ì› ëª©ë¡)
            exam_paper_type="unknown",  # ë¶„ì„ ì‹œ ìë™ íŒë‹¨
        )

        # ë¶„ì„ ëª¨ë“œì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if is_questions_only:
            dynamic_prompt = self._get_questions_only_prompt()
        else:
            dynamic_prompt = self._get_unified_prompt()

        # ============ íŒ¨í„´ ì‹œìŠ¤í…œ ì „ì²´ í†µí•© ============
        await update_step(2)
        print("[Step 2] íŒ¨í„´ ë° ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì¤‘...")
        all_additions = []
        detected_paper_type = "unknown"  # 1ì°¨ ë¶„ë¥˜ ê²°ê³¼ (ìºì‹œìš©)

        # 1. learned_patterns í…Œì´ë¸”: í•™ìŠµëœ ì¸ì‹ ê·œì¹™
        try:
            from app.services.ai_learning import AILearningService
            learning_service = AILearningService(db)
            learned_additions = await learning_service.get_dynamic_prompt_additions()
            if learned_additions:
                all_additions.append(learned_additions)
                print(f"[Pattern] í•™ìŠµ íŒ¨í„´ ì¶”ê°€ë¨ ({len(learned_additions)}ì)")
        except Exception as e:
            print(f"[Pattern Error] learned_patterns: {e}")

        # 2. error_patterns í…Œì´ë¸”: ì˜¤ë¥˜ íŒ¨í„´ (ë¹ˆë„ ë†’ì€ ìƒìœ„ íŒ¨í„´ + ìƒì„¸ ì •ë³´)
        try:
            result = await db.table("error_patterns").select(
                "name, error_type, frequency, feedback_message, feedback_detail, wrong_examples, detection_keywords"
            ).eq("is_active", True).order(
                "occurrence_count", desc=True
            ).limit(15).execute()

            if result.data:
                error_prompt_parts = ["\n## [ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ íŒ¨í„´ - AI ë¶„ì„ ì‹œ ì°¸ê³ ]"]
                for pattern in result.data:
                    part = f"\n### {pattern.get('name', '')} ({pattern.get('error_type', '')})"
                    part += f"\n- í”¼ë“œë°±: {pattern.get('feedback_message', '')}"
                    if pattern.get('feedback_detail'):
                        part += f"\n- ìƒì„¸: {pattern.get('feedback_detail', '')}"
                    # ì˜¤ë‹µ ì˜ˆì‹œ ì¶”ê°€
                    wrong_examples = pattern.get('wrong_examples') or []
                    if wrong_examples and len(wrong_examples) > 0:
                        ex = wrong_examples[0]
                        if isinstance(ex, dict):
                            part += f"\n- ì˜ˆì‹œ: {ex.get('problem', '')} â†’ ì˜¤ë‹µ: {ex.get('wrong_answer', '')}"
                    error_prompt_parts.append(part)
                error_additions = "\n".join(error_prompt_parts)
                all_additions.append(error_additions)
                print(f"[Pattern] ì˜¤ë¥˜ íŒ¨í„´ {len(result.data)}ê°œ ì¶”ê°€ë¨")
        except Exception as e:
            print(f"[Pattern Error] error_patterns: {e}")

        # 3. prompt_templates í…Œì´ë¸”: ëª¨ë“  ìœ í˜•ì˜ í…œí”Œë¦¿ í™œìš©
        try:
            # 3-1. ê¸°ë³¸ ë¶„ì„ ê°€ì´ë“œ (analysis_guide)
            result = await db.table("prompt_templates").select(
                "name, content, template_type, conditions"
            ).eq("is_active", True).in_(
                "template_type", ["analysis_guide", "error_detection"]
            ).order("priority", desc=True).limit(5).execute()

            if result.data:
                for t in result.data:
                    # ì¡°ê±´ ê¸°ë°˜ í•„í„°ë§ (ì‹œí—˜ì§€ ìœ í˜•)
                    conditions = t.get("conditions") or {}
                    cond_paper_type = conditions.get("exam_paper_type")

                    # ì¡°ê±´ì´ ì—†ê±°ë‚˜ unknownì´ë©´ í•­ìƒ í¬í•¨
                    if not cond_paper_type or cond_paper_type == "unknown":
                        all_additions.append(f"\n## [{t.get('name', '')}]\n{t.get('content', '')}")
                print(f"[Pattern] ë¶„ì„ ê°€ì´ë“œ í…œí”Œë¦¿ {len(result.data)}ê°œ ë¡œë“œë¨")

            # 3-2. ë‹¨ì›ë³„ ê°€ì´ë“œ (topic_guide) - í•™ë…„/ë‹¨ì› ê¸°ë°˜
            if grade_level or unit:
                topic_result = await db.table("prompt_templates").select(
                    "name, content, conditions"
                ).eq("is_active", True).eq(
                    "template_type", "topic_guide"
                ).order("priority", desc=True).execute()

                if topic_result.data:
                    for t in topic_result.data:
                        conditions = t.get("conditions") or {}
                        cond_topic = conditions.get("topic", "").lower()

                        # ë‹¨ì› ë§¤ì¹­ (ë¶€ë¶„ ì¼ì¹˜)
                        unit_lower = (unit or "").lower()
                        if not cond_topic or cond_topic in unit_lower or unit_lower in cond_topic:
                            all_additions.append(f"\n## [ë‹¨ì› ê°€ì´ë“œ: {t.get('name', '')}]\n{t.get('content', '')}")
                            print(f"[Pattern] ë‹¨ì› ê°€ì´ë“œ '{t.get('name', '')}' ì¶”ê°€ë¨")
                            break  # ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ 1ê°œë§Œ

            # 3-3. êµìœ¡ê³¼ì • ê°€ì´ë“œ (curriculum_guide) - 22ê°œì •
            curriculum_result = await db.table("prompt_templates").select(
                "name, content"
            ).eq("is_active", True).eq(
                "template_type", "curriculum_guide"
            ).order("priority", desc=True).limit(2).execute()

            if curriculum_result.data:
                for t in curriculum_result.data:
                    all_additions.append(f"\n## [êµìœ¡ê³¼ì • ê°€ì´ë“œ: {t.get('name', '')}]\n{t.get('content', '')}")
                print(f"[Pattern] êµìœ¡ê³¼ì • ê°€ì´ë“œ {len(curriculum_result.data)}ê°œ ì¶”ê°€ë¨")

            # 3-4. í”¼ë“œë°± í…œí”Œë¦¿ (feedback) - ì½”ë©˜íŠ¸ ì‘ì„± ì°¸ê³ ìš©
            feedback_result = await db.table("prompt_templates").select(
                "name, content"
            ).eq("is_active", True).eq(
                "template_type", "feedback"
            ).order("priority", desc=True).limit(2).execute()

            if feedback_result.data:
                feedback_content = "\n## [AI ì½”ë©˜íŠ¸ ì‘ì„± ê°€ì´ë“œ]\n"
                for t in feedback_result.data:
                    feedback_content += f"\n### {t.get('name', '')}\n{t.get('content', '')}"
                all_additions.append(feedback_content)
                print(f"[Pattern] í”¼ë“œë°± í…œí”Œë¦¿ {len(feedback_result.data)}ê°œ ì¶”ê°€ë¨")

        except Exception as e:
            print(f"[Pattern Error] prompt_templates: {e}")

        # 4. problem_categories + problem_types: í† í”½ ë¶„ë¥˜ ê°€ì´ë“œ
        try:
            # exam_scopeì—ì„œ ë‹¨ì› í‚¤ì›Œë“œ ì¶”ì¶œ (í•„í„°ë§ìš©)
            scope_keywords: set[str] = set()
            if exam_scope:
                for scope in exam_scope:
                    # "ì¤‘1 > ìˆ˜ì™€ ì—°ì‚° > ì •ìˆ˜ì™€ ìœ ë¦¬ìˆ˜" í˜•ì‹ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    parts = scope.split(" > ")
                    for part in parts:
                        # í•™ë…„ ì •ë³´ ì œì™¸ (ì¤‘1, ê³ 2 ë“±)
                        if not any(g in part for g in ["ì¤‘1", "ì¤‘2", "ì¤‘3", "ê³ 1", "ê³ 2", "ê³ 3"]):
                            scope_keywords.add(part.strip())
                print(f"[Pattern] exam_scope í‚¤ì›Œë“œ: {scope_keywords}")

            # í™œì„±í™”ëœ ì¹´í…Œê³ ë¦¬ì™€ ìœ í˜• ë¡œë“œ
            cat_result = await db.table("problem_categories").select(
                "id, name, description"
            ).eq("is_active", True).order("display_order").execute()

            if cat_result.data:
                topic_guide_parts = ["\n## [ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ ê°€ì´ë“œ - DB ê¸°ë°˜]"]
                matched_count = 0

                for cat in cat_result.data:
                    cat_name = cat.get("name", "")
                    cat_desc = cat.get("description", "")

                    # exam_scopeê°€ ìˆìœ¼ë©´ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ë§Œ í¬í•¨
                    if scope_keywords:
                        is_relevant = any(
                            kw in cat_name or kw in cat_desc
                            for kw in scope_keywords
                        )
                        if not is_relevant:
                            continue

                    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì œ ìœ í˜• ë¡œë“œ
                    types_result = await db.table("problem_types").select(
                        "name, keywords, core_concepts, grade_levels"
                    ).eq("category_id", cat["id"]).eq("is_active", True).order("display_order").limit(10).execute()

                    if types_result.data:
                        cat_part = f"\n### [{cat_name}] {cat_desc}"
                        for pt in types_result.data:
                            keywords = pt.get("keywords") or []
                            grades = pt.get("grade_levels") or []
                            cat_part += f"\n- {pt.get('name', '')}"
                            if keywords:
                                cat_part += f" (í‚¤ì›Œë“œ: {', '.join(keywords[:3])})"
                            if grades:
                                cat_part += f" [{', '.join(grades)}]"
                        topic_guide_parts.append(cat_part)
                        matched_count += 1

                if len(topic_guide_parts) > 1:
                    all_additions.append("\n".join(topic_guide_parts))
                    if scope_keywords:
                        print(f"[Pattern] ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ ê°€ì´ë“œ {matched_count}ê°œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ë¨ (ì „ì²´ {len(cat_result.data)}ê°œ ì¤‘ í•„í„°ë§)")
                    else:
                        print(f"[Pattern] ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ ê°€ì´ë“œ {len(cat_result.data)}ê°œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ë¨")
        except Exception as e:
            print(f"[Pattern Error] problem_categories/types: {e}")

        # 5. ì‹œí—˜ ìœ í˜•ë³„ ê°€ì´ë“œ (exam_type_guide) - ìˆ˜ëŠ¥/ë‚´ì‹  êµ¬ë¶„
        # questions_only ëª¨ë“œì—ì„œëŠ” ì±„ì  ê´€ë ¨ ê°€ì´ë“œ ë¶ˆí•„ìš”
        if not is_questions_only:
            try:
                exam_type_result = await db.table("prompt_templates").select(
                    "name, content, conditions"
                ).eq("is_active", True).eq(
                    "template_type", "exam_type_guide"
                ).order("priority", desc=True).execute()

                if exam_type_result.data:
                    # ëª¨ë“  ì‹œí—˜ ìœ í˜• ê°€ì´ë“œ ì¶”ê°€ (ì¡°ê±´ ë¬´ì‹œ - ë¶„ì„ ì‹œ AIê°€ íŒë‹¨)
                    for t in exam_type_result.data:
                        all_additions.append(f"\n## [ì‹œí—˜ ìœ í˜• ì°¸ê³ : {t.get('name', '')}]\n{t.get('content', '')}")
                    print(f"[Pattern] ì‹œí—˜ ìœ í˜• ê°€ì´ë“œ {len(exam_type_result.data)}ê°œ ì¶”ê°€ë¨")
            except Exception as e:
                print(f"[Pattern Error] exam_type_guide: {e}")
        else:
            print("[Pattern] questions_only ëª¨ë“œ - ì‹œí—˜ ìœ í˜• ê°€ì´ë“œ ê±´ë„ˆëœ€")

        # ëª¨ë“  íŒ¨í„´ ì •ë³´ ë³‘í•©
        combined_additions = "\n\n".join(all_additions) if all_additions else ""
        print(f"[Pattern] ì´ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ê¸¸ì´: {len(combined_additions)}ì")

        # ============ 2ë‹¨ê³„ ë¶„ì„ ì‹œìŠ¤í…œ ============
        marks_result = {"marks": [], "overall_grading_status": "unknown"}

        # [1ë‹¨ê³„] ì±„ì  í‘œì‹œ íƒì§€ (questions_only ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆëœ€)
        if is_questions_only:
            print("[Step 2-1] ë¬¸í•­ë§Œ ë¶„ì„ ëª¨ë“œ - ì±„ì  í‘œì‹œ íƒì§€ ê±´ë„ˆëœ€")
        else:
            print("[Step 2-1] ì±„ì  í‘œì‹œ íƒì§€ ì¤‘ (1ë‹¨ê³„ ë¶„ì„)...")
            marks_result = await self.detect_grading_marks(file_path)

            grading_context = ""
            if marks_result.get("marks"):
                print(f"[Step 2-1] {len(marks_result['marks'])}ê°œ ì±„ì  í‘œì‹œ íƒì§€ë¨")
                grading_context = self._build_grading_context_from_marks(marks_result)
                combined_additions += grading_context
            else:
                print("[Step 2-1] ì±„ì  í‘œì‹œ ì—†ìŒ ë˜ëŠ” íƒì§€ ì‹¤íŒ¨")

        # [2ë‹¨ê³„] AI ë¶„ì„ ì‹¤í–‰
        # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë¶„ì„ ì‹œì‘ ì‹œì ì— step ì—…ë°ì´íŠ¸
        await update_step(3)
        mode_label = "ë¬¸í•­ ë¶„ì„" if is_questions_only else "í†µí•© ë¶„ì„"
        print(f"[Step 2-2] AI {mode_label} ì‹¤í–‰ ì¤‘...")
        result = await self.analyze_exam_file(
            file_path=file_path,
            dynamic_prompt_additions=combined_additions,
            exam_type="unified" if not is_questions_only else "blank",
            custom_prompt=dynamic_prompt,
            subject=subject,
        )

        # [í›„ì²˜ë¦¬] êµì°¨ ê²€ì¦ (questions_only ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆëœ€)
        if not is_questions_only:
            result = self._cross_validate_grading(result, marks_result)

        # [í›„ì²˜ë¦¬] ê³¼ëª© í†µí•© (60% ê·œì¹™ ì ìš©)
        result = self._consolidate_dominant_topic(result, threshold=0.6)

        # 3. ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ
        paper_type = result.get("paper_type", "blank")
        grading_status = result.get("grading_status", "not_graded")

        print(f"  - ìœ í˜•: {paper_type}")
        print(f"  - ì±„ì  ìƒíƒœ: {grading_status}")

        # ì¡°ê±´ë¶€ í…œí”Œë¦¿ ë¡œë“œ (questions_only ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆëœ€ - ì´ë¯¸ ë¶„ì„ì´ ëë‚¬ìœ¼ë¯€ë¡œ ì˜ë¯¸ ì—†ìŒ)
        if not is_questions_only:
            try:
                conditional_result = await db.table("prompt_templates").select(
                    "name, content, conditions"
                ).eq("is_active", True).order("priority", desc=True).execute()

                if conditional_result.data:
                    for t in conditional_result.data:
                        conditions = t.get("conditions") or {}
                        cond_paper_type = conditions.get("exam_paper_type")

                        # ì¡°ê±´ì´ í˜„ì¬ ë¶„ë¥˜ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ë©´ í”¼ë“œë°±ì— í™œìš©
                        if cond_paper_type and cond_paper_type == paper_type:
                            print(f"[Pattern] ì¡°ê±´ë¶€ í…œí”Œë¦¿ '{t.get('name', '')}' ì ìš©ë¨ (paper_type={paper_type})")
                            # ê²°ê³¼ì— ì ìš©ëœ í…œí”Œë¦¿ ì •ë³´ ê¸°ë¡
                            if "_applied_templates" not in result:
                                result["_applied_templates"] = []
                            result["_applied_templates"].append(t.get("name", ""))
            except Exception as e:
                print(f"[Pattern Error] conditional templates: {e}")

        # exam_type ê²°ì • (í›„ì²˜ë¦¬ìš©)
        if paper_type in ["answered", "mixed"]:
            exam_type = "student"
        else:
            exam_type = "blank"

        # ë¶„ë¥˜ ê²°ê³¼ë¥¼ _classificationì— ì €ì¥
        result["_classification"] = {
            "paper_type": paper_type,
            "paper_type_confidence": result.get("paper_type_confidence", 0.9),
            "grading_status": grading_status,
            "indicators": result.get("paper_type_indicators", []),
            "grading_indicators": result.get("grading_indicators", []),
        }

        # 4. íŒ¨í„´ ë§¤ì¹­ (í–¥í›„ êµ¬í˜„)
        # TODO: ë¶„ì„ ê²°ê³¼ì—ì„œ íŒ¨í„´ ë§¤ì¹­ í›„ PatternMatchHistoryì— ê¸°ë¡

        # 5. Category ê²€ì¦ ë° ìë™ ìˆ˜ì • (topicì˜ ê³¼ëª©ëª…ì´ ì„ íƒí•œ categoryì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸)
        if category:
            result = self._validate_and_fix_category(result, category)

        # ============ ê²°ê³¼ ìºì‹± ============
        elapsed = time.time() - start_time
        result["_cache_hit"] = False
        result["_elapsed_seconds"] = round(elapsed, 2)

        if file_hash:
            cache_key = compute_analysis_cache_key(file_hash, grade_level, unit)
            cache.set(cache_key, result)
            print(f"[Cache SAVE] {cache_key[:20]}... ({elapsed:.2f}ì´ˆ)")
            print(f"[Cache Stats] {cache.get_stats()}")

        # Analytics ë¡œê¹…: ë¶„ì„ ì™„ë£Œ
        if user_id and exam_id:
            try:
                from app.services.analytics_log import get_analytics_log_service
                analytics = get_analytics_log_service(db)

                questions = result.get("questions", [])
                summary = result.get("summary", {})

                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = {
                    "duration_seconds": round(elapsed, 2),
                    "detected_questions": len(questions),
                    "avg_confidence": result.get("avg_confidence"),
                    "total_points": sum(q.get("points", 0) for q in questions),
                }

                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                metadata = {
                    "paper_type": paper_type,
                    "exam_type": exam_type,
                    "grading_status": grading_status,
                    "grade_level": grade_level,
                    "unit": unit,
                    "analysis_mode": analysis_mode,
                    "difficulty_distribution": summary.get("difficulty_distribution", {}),
                    "type_distribution": summary.get("type_distribution", {}),
                }

                # ë¶„ì„ IDëŠ” ë‚˜ì¤‘ì— analysis.pyì—ì„œ ìƒì„±ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” exam_idë§Œ ì‚¬ìš©
                await analytics.log_analysis_complete(
                    user_id=user_id,
                    exam_id=exam_id,
                    analysis_id=exam_id,  # ì„ì‹œë¡œ exam_id ì‚¬ìš©, analysis.pyì—ì„œ ì—…ë°ì´íŠ¸ í•„ìš”
                    metrics=metrics,
                    metadata=metadata,
                )
            except Exception as e:
                print(f"[Analytics Log Error] {e}")

        return result

    # ============================================
    # 3-2. ì •ì˜¤ë‹µ ë¶„ì„ ì „ìš© (2ë‹¨ê³„ ë¶„ì„)
    # ============================================
    async def analyze_answers_only(
        self,
        db: SupabaseClient,
        file_path: str,
        existing_questions: list[dict],
        exam_id: str | None = None,
    ) -> dict:
        """ê¸°ì¡´ ë¬¸í•­ì— ëŒ€í•´ ì •ì˜¤ë‹µë§Œ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            existing_questions: ê¸°ì¡´ ë¶„ì„ëœ ë¬¸í•­ ëª©ë¡
            exam_id: ì‹œí—˜ì§€ ID (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©)

        Returns:
            ì •ì˜¤ë‹µ ë¶„ì„ ê²°ê³¼ (questions ë°°ì—´ë§Œ í¬í•¨)
        """
        if not self.client:
            raise Exception("AI service is not configured")

        start_time = time.time()

        # í—¬í¼: ë¶„ì„ ë‹¨ê³„ ì—…ë°ì´íŠ¸
        async def update_step(step: int):
            if exam_id and db:
                try:
                    await db.table("exams").eq("id", exam_id).update({"analysis_step": step}).execute()
                except Exception as e:
                    print(f"[Step Update Error] {e}")

        # 1. ì±„ì  í‘œì‹œ íƒì§€
        await update_step(2)
        print("[Answer Analysis Step 1] ì±„ì  í‘œì‹œ íƒì§€ ì¤‘...")
        marks_result = await self.detect_grading_marks(file_path)

        grading_context = ""
        if marks_result.get("marks"):
            print(f"[Answer Analysis Step 1] {len(marks_result['marks'])}ê°œ ì±„ì  í‘œì‹œ íƒì§€ë¨")
            grading_context = self._build_grading_context_from_marks(marks_result)

        # 2. ê¸°ì¡´ ë¬¸í•­ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        questions_context = self._build_questions_context(existing_questions)

        # 3. ì •ì˜¤ë‹µ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        await update_step(3)
        print("[Answer Analysis Step 2] ì •ì˜¤ë‹µ ë¶„ì„ ì¤‘...")
        prompt = self._get_answers_only_prompt(questions_context, grading_context)

        # 4. AI ë¶„ì„ ì‹¤í–‰
        # ê³¼ëª© ì •ë³´ëŠ” ê¸°ì¡´ ë¬¸í•­ì—ì„œ ì¶”ë¡  (topicì˜ ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸)
        inferred_subject = "ìˆ˜í•™"
        if existing_questions:
            first_topic = existing_questions[0].get("topic", "")
            if "ì˜ì–´" in first_topic or "ë¬¸ë²•" in first_topic:
                inferred_subject = "ì˜ì–´"

        result = await self.analyze_exam_file(
            file_path=file_path,
            dynamic_prompt_additions="",
            exam_type="student",
            custom_prompt=prompt,
            subject=inferred_subject,
        )

        # 5. ì±„ì  ê²°ê³¼ êµì°¨ ê²€ì¦
        result = self._cross_validate_grading(result, marks_result)

        elapsed = time.time() - start_time
        print(f"[Answer Analysis] ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")

        return result

    def _build_questions_context(self, questions: list[dict]) -> str:
        """ê¸°ì¡´ ë¬¸í•­ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
        lines = ["## ê¸°ì¡´ ë¶„ì„ëœ ë¬¸í•­ ëª©ë¡\n"]
        for q in questions:
            q_num = q.get("question_number", "?")
            points = q.get("points", "?")
            topic = q.get("topic", "ë¯¸ë¶„ë¥˜")
            lines.append(f"- ë¬¸í•­ {q_num}: ë°°ì  {points}ì , {topic}")
        return "\n".join(lines)

    def _get_answers_only_prompt(self, questions_context: str, grading_context: str) -> str:
        """ì •ì˜¤ë‹µ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸."""
        return f"""ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™êµ ìˆ˜í•™ ì‹œí—˜ì§€ ì±„ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ëª©í‘œ: ì •ì˜¤ë‹µ ë¶„ì„ë§Œ ìˆ˜í–‰

ì´ ì‹œí—˜ì§€ëŠ” ì´ë¯¸ ë¬¸í•­ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
**í•™ìƒì˜ ë‹µì•ˆê³¼ ì±„ì  í‘œì‹œë§Œ ë¶„ì„**í•˜ì„¸ìš”.

{questions_context}

{grading_context}

## ë¶„ì„í•  ë‚´ìš©

ê° ë¬¸í•­ì— ëŒ€í•´:
1. **is_correct**: ì •ë‹µ ì—¬ë¶€ (true/false/null)
   - ì±„ì  í‘œì‹œ ì—†ìœ¼ë©´ **ë°˜ë“œì‹œ null** (ì¶”ì¸¡ ê¸ˆì§€!)
2. **student_answer**: í•™ìƒì´ ì‘ì„±í•œ ë‹µ
3. **earned_points**: íšë“ ì ìˆ˜
4. **error_type**: ì˜¤ë‹µì¼ ê²½ìš° ì˜¤ë¥˜ ìœ í˜•
5. **grading_rationale**: íŒì • ê·¼ê±°

## âš ï¸ í•µì‹¬ ê·œì¹™

1. **ì±„ì  í‘œì‹œê°€ ì—†ìœ¼ë©´ is_correct = null**
   - í•™ìƒì´ ë‹µì„ ì¼ì–´ë„ O/X í‘œì‹œ ì—†ìœ¼ë©´ ë¯¸ì±„ì !
   - ì ˆëŒ€ë¡œ ì •ë‹µì„ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”

2. **ì±„ì  í‘œì‹œ ì¸ì‹**
   - O, â—‹, âœ“ â†’ ì •ë‹µ (is_correct: true)
   - X, âœ—, / â†’ ì˜¤ë‹µ (is_correct: false)
   - ë¬¸ì œë²ˆí˜¸ì— ë™ê·¸ë¼ë¯¸ â†’ ì •ë‹µ!
   - ë¬¸ì œë²ˆí˜¸ì— X/ë¹—ê¸ˆ â†’ ì˜¤ë‹µ!

## JSON ì¶œë ¥ í˜•ì‹

{{
    "questions": [
        {{
            "question_number": 1,
            "is_correct": true,
            "student_answer": "â‘¢",
            "earned_points": 3,
            "error_type": null,
            "grading_rationale": "ë‹µ â‘¢ì— Oí‘œì‹œ í™•ì¸"
        }},
        {{
            "question_number": 2,
            "is_correct": false,
            "student_answer": "â‘ ",
            "earned_points": 0,
            "error_type": "careless_mistake",
            "grading_rationale": "ë‹µì•ˆì— Xí‘œì‹œ, ì •ë‹µì€ â‘£"
        }},
        {{
            "question_number": 3,
            "is_correct": null,
            "student_answer": "5",
            "earned_points": null,
            "error_type": null,
            "grading_rationale": "ì±„ì  í‘œì‹œ ì—†ìŒ"
        }}
    ]
}}

## error_type ê°’
- calculation_error: ê³„ì‚° ì‹¤ìˆ˜
- concept_error: ê°œë… ì˜¤í•´
- careless_mistake: ë‹¨ìˆœ ì‹¤ìˆ˜
- process_error: í’€ì´ ê³¼ì • ì˜¤ë¥˜
- incomplete: ë¯¸ì™„ì„±

ëª¨ë“  ë¬¸í•­ì˜ ì •ì˜¤ë‹µì„ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""

    # ============================================
    # 3-3. ìµœì í™”ëœ ì •ì˜¤ë‹µ ë¶„ì„ (í† í° ì ˆì•½ + ì •í™•ë„ í–¥ìƒ)
    # ============================================

    # ê³ ì‹ ë¢°ë„ íƒì§€ ì„ê³„ê°’ (ì´ ì´ìƒì´ë©´ AI ë¶„ì„ ìŠ¤í‚µ)
    # í•™ìƒ ìê°€ì±„ì ì€ ë„ˆì €ë¶„í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0.85ë¡œ ì„¤ì •
    HIGH_CONFIDENCE_THRESHOLD = 0.85

    def _classify_questions_by_detection(
        self,
        marks_result: dict,
        existing_questions: list[dict],
    ) -> tuple[list[dict], list[int]]:
        """
        [ë°©ì•ˆ 1] íƒì§€ ìš°ì„  ë¶„ê¸° - ê³ ì‹ ë¢°ë„ íƒì§€ ê²°ê³¼ëŠ” AI ë¶„ì„ ìŠ¤í‚µ

        Args:
            marks_result: ì±„ì  í‘œì‹œ íƒì§€ ê²°ê³¼
            existing_questions: ê¸°ì¡´ ë¶„ì„ëœ ë¬¸í•­ ëª©ë¡

        Returns:
            (resolved_questions, uncertain_question_numbers)
            - resolved_questions: íƒì§€ë§Œìœ¼ë¡œ ê²°ì •ëœ ë¬¸í•­ë“¤ (AI ë¶„ì„ ë¶ˆí•„ìš”)
            - uncertain_question_numbers: AI ë¶„ì„ì´ í•„ìš”í•œ ë¬¸í•­ ë²ˆí˜¸ë“¤
        """
        marks = marks_result.get("marks", [])
        if not marks:
            # íƒì§€ ê²°ê³¼ ì—†ìŒ â†’ ì „ì²´ AI ë¶„ì„ í•„ìš”
            return [], [q.get("question_number") for q in existing_questions]

        # íƒì§€ ê²°ê³¼ë¥¼ ë¬¸í•­ë²ˆí˜¸ë¡œ ì¸ë±ì‹±
        marks_by_num = {}
        for m in marks:
            q_num = m.get("question_number")
            if q_num:
                marks_by_num[q_num] = m

        resolved_questions = []
        uncertain_numbers = []

        for q in existing_questions:
            q_num = q.get("question_number")
            if q_num not in marks_by_num:
                # íƒì§€ ê²°ê³¼ ì—†ìŒ â†’ AI ë¶„ì„ í•„ìš”
                uncertain_numbers.append(q_num)
                continue

            mark = marks_by_num[q_num]
            confidence = mark.get("confidence", 0)
            indicates = mark.get("indicates")

            # ê³ ì‹ ë¢°ë„ + ëª…í™•í•œ íŒì • â†’ AI ë¶„ì„ ìŠ¤í‚µ
            if confidence >= self.HIGH_CONFIDENCE_THRESHOLD and indicates in ["correct", "incorrect"]:
                resolved_q = {
                    "question_number": q_num,
                    "is_correct": indicates == "correct",
                    "earned_points": q.get("points", 0) if indicates == "correct" else 0,
                    "student_answer": mark.get("student_answer"),
                    "error_type": None if indicates == "correct" else "unknown",
                    "grading_rationale": f"ê³ ì‹ ë¢°ë„ íƒì§€ ({confidence:.0%}): {mark.get('mark_symbol', '')} í‘œì‹œ",
                    "_resolved_by": "detection",
                    "_detection_confidence": confidence,
                }
                resolved_questions.append(resolved_q)
                print(f"[Optimized] Q{q_num}: íƒì§€ë¡œ í•´ê²° ({indicates}, {confidence:.0%})")
            else:
                # ì €ì‹ ë¢°ë„ ë˜ëŠ” ë¶ˆí™•ì‹¤ â†’ AI ë¶„ì„ í•„ìš”
                uncertain_numbers.append(q_num)

        return resolved_questions, uncertain_numbers

    def _apply_score_based_validation(
        self,
        marks_result: dict,
        existing_questions: list[dict],
    ) -> list[dict]:
        """
        [ë°©ì•ˆ 5] ë°°ì  ê¸°ë°˜ ê²€ì¦ - ì ìˆ˜ë¡œ ì •ì˜¤ë‹µ íŒì • (Zero-token)

        íƒì§€ëœ ì ìˆ˜ì™€ ë°°ì ì„ ë¹„êµí•˜ì—¬ AI ì—†ì´ íŒì •:
        - ì ìˆ˜ == ë°°ì  â†’ ì •ë‹µ
        - ì ìˆ˜ == 0 â†’ ì˜¤ë‹µ
        - 0 < ì ìˆ˜ < ë°°ì  â†’ ë¶€ë¶„ì ìˆ˜ (ì˜¤ë‹µ)

        Returns:
            score_resolved_questions: ì ìˆ˜ë¡œ íŒì •ëœ ë¬¸í•­ë“¤
        """
        marks = marks_result.get("marks", [])
        if not marks:
            return []

        # ì ìˆ˜ ê¸°ì¬ëœ ë§ˆí¬ë§Œ í•„í„°ë§
        score_marks = {}
        for m in marks:
            if m.get("mark_type") == "score" and m.get("mark_symbol"):
                q_num = m.get("question_number")
                try:
                    # "3", "3ì ", "2/4" ë“± íŒŒì‹±
                    score_str = str(m.get("mark_symbol", "")).replace("ì ", "").strip()
                    if "/" in score_str:
                        # ë¶€ë¶„ì ìˆ˜: "2/4" í˜•íƒœ
                        earned, total = score_str.split("/")
                        score_marks[q_num] = {
                            "earned": float(earned),
                            "total": float(total),
                            "confidence": m.get("confidence", 0),
                        }
                    else:
                        score_marks[q_num] = {
                            "earned": float(score_str),
                            "total": None,
                            "confidence": m.get("confidence", 0),
                        }
                except (ValueError, TypeError):
                    continue

        if not score_marks:
            return []

        # ê¸°ì¡´ ë¬¸í•­ê³¼ ë§¤ì¹­í•˜ì—¬ íŒì •
        questions_by_num = {q.get("question_number"): q for q in existing_questions}
        resolved = []

        for q_num, score_info in score_marks.items():
            if q_num not in questions_by_num:
                continue

            q = questions_by_num[q_num]
            points = q.get("points", 0)
            earned = score_info["earned"]
            confidence = score_info["confidence"]

            # ì‹ ë¢°ë„ ì²´í¬
            if confidence < 0.75:
                continue

            # íŒì •
            if earned == points:
                # ë§Œì  = ì •ë‹µ
                resolved.append({
                    "question_number": q_num,
                    "is_correct": True,
                    "earned_points": earned,
                    "error_type": None,
                    "grading_rationale": f"ì ìˆ˜ ê¸°ì¬ í™•ì¸: {earned}ì  (ë§Œì )",
                    "_resolved_by": "score_validation",
                })
                print(f"[Score-Based] Q{q_num}: ë§Œì  ({earned}/{points})")
            elif earned == 0:
                # 0ì  = ì˜¤ë‹µ
                resolved.append({
                    "question_number": q_num,
                    "is_correct": False,
                    "earned_points": 0,
                    "error_type": "unknown",
                    "grading_rationale": f"ì ìˆ˜ ê¸°ì¬ í™•ì¸: 0ì ",
                    "_resolved_by": "score_validation",
                })
                print(f"[Score-Based] Q{q_num}: 0ì  (ì˜¤ë‹µ)")
            elif 0 < earned < points:
                # ë¶€ë¶„ì ìˆ˜ = ê°ì ë¨ (ì˜¤ë‹µ ì²˜ë¦¬)
                resolved.append({
                    "question_number": q_num,
                    "is_correct": False,
                    "earned_points": earned,
                    "error_type": "process_error",
                    "grading_rationale": f"ë¶€ë¶„ì ìˆ˜ ê¸°ì¬: {earned}/{points}ì ",
                    "_resolved_by": "score_validation",
                })
                print(f"[Score-Based] Q{q_num}: ë¶€ë¶„ì ìˆ˜ ({earned}/{points})")

        return resolved

    def _get_optimized_answers_prompt(
        self,
        uncertain_questions: list[dict],
        grading_context: str,
    ) -> str:
        """
        [ë°©ì•ˆ 2] ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ - ë¶ˆí™•ì‹¤ ë¬¸í•­ë§Œ AI ë¶„ì„ ìš”ì²­

        í† í° ì ˆì•½ì„ ìœ„í•´:
        - í™•ì‹¤í•œ ë¬¸í•­ì€ ì œì™¸
        - ìµœì†Œí•œì˜ í•„ë“œë§Œ ìš”ì²­
        - ê°„ê²°í•œ ì¶œë ¥ í˜•ì‹
        """
        if not uncertain_questions:
            return ""

        q_list = ", ".join(str(q.get("question_number")) for q in uncertain_questions)
        q_context = "\n".join([
            f"- {q.get('question_number')}ë²ˆ: ë°°ì  {q.get('points', '?')}ì "
            for q in uncertain_questions
        ])

        return f"""## ì •ì˜¤ë‹µ ë¶„ì„ (ë¶ˆí™•ì‹¤ ë¬¸í•­ë§Œ)

ë‹¤ìŒ ë¬¸í•­ë“¤ì˜ ì •ì˜¤ë‹µì„ íŒì •í•´ì£¼ì„¸ìš”: [{q_list}]

{q_context}

{grading_context}

## ê·œì¹™
1. **ì±„ì  í‘œì‹œ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ is_correct: null** (ì¶”ì¸¡ ê¸ˆì§€!)
2. ë¬¸í•­ë²ˆí˜¸(1. 2. 3.)ì— O/ë™ê·¸ë¼ë¯¸ = ì •ë‹µ
3. ë¬¸í•­ë²ˆí˜¸ì— ì‚¬ì„ (/) = ì˜¤ë‹µ
4. ë³´ê¸°ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì˜ ë™ê·¸ë¼ë¯¸ = í•™ìƒ ë‹µ (ì±„ì  ì•„ë‹˜!)
5. **ì„œìˆ í˜•ì€ ëŒ€ë¶€ë¶„ ë¯¸ì±„ì  â†’ null ì²˜ë¦¬**

## JSON ì¶œë ¥
{{"questions": [
  {{"n": ë¬¸í•­ë²ˆí˜¸, "c": true/false/null, "e": íšë“ì ìˆ˜, "r": "íŒì •ê·¼ê±°"}}
]}}

ì˜ˆì‹œ:
{{"questions": [
  {{"n": 3, "c": true, "e": 4, "r": "ë¬¸í•­ë²ˆí˜¸ì— O"}},
  {{"n": 7, "c": false, "e": 0, "r": "ë¬¸í•­ë²ˆí˜¸ì— ì‚¬ì„ "}},
  {{"n": 12, "c": null, "e": null, "r": "ì±„ì í‘œì‹œì—†ìŒ"}}
]}}
"""

    async def analyze_answers_optimized(
        self,
        db: SupabaseClient,
        file_path: str,
        existing_questions: list[dict],
        exam_id: str | None = None,
    ) -> dict:
        """
        [ìµœì í™”ëœ ì •ì˜¤ë‹µ ë¶„ì„]

        í† í° ì ˆì•½ + ì •í™•ë„ í–¥ìƒ ì „ëµ:
        1. ì±„ì  í‘œì‹œ íƒì§€ (ê¸°ì¡´)
        2. ë°°ì  ê¸°ë°˜ ê²€ì¦ - ì ìˆ˜ë¡œ ë°”ë¡œ íŒì • (Zero-token)
        3. íƒì§€ ìš°ì„  ë¶„ê¸° - ê³ ì‹ ë¢°ë„ ê²°ê³¼ëŠ” AI ìŠ¤í‚µ
        4. AI ë¶„ì„ - ë¶ˆí™•ì‹¤ ë¬¸í•­ë§Œ ìš”ì²­ (í† í° ì ˆì•½)
        5. ê²°ê³¼ ë³‘í•© + êµì°¨ ê²€ì¦

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            existing_questions: ê¸°ì¡´ ë¶„ì„ëœ ë¬¸í•­ ëª©ë¡
            exam_id: ì‹œí—˜ì§€ ID

        Returns:
            ìµœì í™”ëœ ì •ì˜¤ë‹µ ë¶„ì„ ê²°ê³¼
        """
        if not self.client:
            raise Exception("AI service is not configured")

        start_time = time.time()
        stats = {
            "total_questions": len(existing_questions),
            "resolved_by_detection": 0,  # O/X íƒì§€ë¡œ í•´ê²°ëœ ë¬¸í•­
            "resolved_by_ai": 0,         # AI ë¶„ì„ì´ í•„ìš”í–ˆë˜ ë¬¸í•­
            "tokens_saved_estimate": 0,
        }

        # í—¬í¼: ë¶„ì„ ë‹¨ê³„ ì—…ë°ì´íŠ¸
        async def update_step(step: int):
            if exam_id and db:
                try:
                    await db.table("exams").eq("id", exam_id).update({"analysis_step": step}).execute()
                except Exception as e:
                    print(f"[Step Update Error] {e}")

        # ========================================
        # 1ë‹¨ê³„: ì±„ì  í‘œì‹œ íƒì§€ (ë¹¨ê°„íœ O/X í‘œì‹œ)
        # ========================================
        await update_step(2)
        print("[Optimized Step 1] ì±„ì  í‘œì‹œ íƒì§€ ì¤‘ (ë¹¨ê°„íœ O/X)...")
        marks_result = await self.detect_grading_marks(file_path)
        marks_count = len(marks_result.get("marks", []))
        print(f"[Optimized Step 1] {marks_count}ê°œ ì±„ì  í‘œì‹œ íƒì§€ë¨")

        # ========================================
        # 2ë‹¨ê³„: O/X íƒì§€ ìš°ì„  ë¶„ê¸°
        # - ê³ ì‹ ë¢°ë„(90%+) ë™ê·¸ë¼ë¯¸/ì‚¬ì„  â†’ AI ìŠ¤í‚µ
        # - ì €ì‹ ë¢°ë„ ë˜ëŠ” ë¶ˆí™•ì‹¤ â†’ AI ë¶„ì„
        # ========================================
        print("[Optimized Step 2] O/X íƒì§€ ìš°ì„  ë¶„ê¸° ì¤‘...")
        detection_resolved, uncertain_nums = self._classify_questions_by_detection(
            marks_result, existing_questions
        )
        stats["resolved_by_detection"] = len(detection_resolved)

        # ========================================
        # 4ë‹¨ê³„: AI ë¶„ì„ (ë¶ˆí™•ì‹¤ ë¬¸í•­ë§Œ)
        # ========================================
        ai_resolved = []
        if uncertain_nums:
            await update_step(3)
            print(f"[Optimized Step 4] AI ë¶„ì„ ì¤‘... ({len(uncertain_nums)}ê°œ ë¬¸í•­)")

            # ë¶ˆí™•ì‹¤ ë¬¸í•­ ì •ë³´ ì¶”ì¶œ
            uncertain_questions = [
                q for q in existing_questions
                if q.get("question_number") in uncertain_nums
            ]

            # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            grading_context = self._build_grading_context_from_marks(marks_result)
            optimized_prompt = self._get_optimized_answers_prompt(
                uncertain_questions, grading_context
            )

            if optimized_prompt:
                # ê³¼ëª© ì •ë³´ëŠ” ê¸°ì¡´ ë¬¸í•­ì—ì„œ ì¶”ë¡ 
                inferred_subject = "ìˆ˜í•™"
                if existing_questions:
                    first_topic = existing_questions[0].get("topic", "")
                    if "ì˜ì–´" in first_topic or "ë¬¸ë²•" in first_topic:
                        inferred_subject = "ì˜ì–´"

                try:
                    result = await self.analyze_exam_file(
                        file_path=file_path,
                        dynamic_prompt_additions="",
                        exam_type="student",
                        custom_prompt=optimized_prompt,
                        subject=inferred_subject,
                    )

                    # ê°„ê²°í•œ í˜•ì‹ íŒŒì‹± (n, c, e, r â†’ ì •ì‹ í•„ë“œ)
                    for q in result.get("questions", []):
                        parsed_q = {
                            "question_number": q.get("n") or q.get("question_number"),
                            "is_correct": q.get("c") if "c" in q else q.get("is_correct"),
                            "earned_points": q.get("e") if "e" in q else q.get("earned_points"),
                            "grading_rationale": q.get("r") or q.get("grading_rationale", ""),
                            "error_type": q.get("error_type"),
                            "_resolved_by": "ai_analysis",
                        }

                        # earned_pointsê°€ nullì´ê³  ì •ë‹µì´ë©´ ë°°ì ìœ¼ë¡œ ì„¤ì •
                        if parsed_q["is_correct"] is True and parsed_q["earned_points"] is None:
                            orig_q = next(
                                (oq for oq in existing_questions
                                 if oq.get("question_number") == parsed_q["question_number"]),
                                None
                            )
                            if orig_q:
                                parsed_q["earned_points"] = orig_q.get("points", 0)

                        ai_resolved.append(parsed_q)

                    stats["resolved_by_ai"] = len(ai_resolved)

                except Exception as e:
                    print(f"[Optimized AI Error] {e}")
                    # AI ì‹¤íŒ¨ ì‹œ nullë¡œ ì²˜ë¦¬
                    for q_num in uncertain_nums:
                        ai_resolved.append({
                            "question_number": q_num,
                            "is_correct": None,
                            "earned_points": None,
                            "grading_rationale": "AI ë¶„ì„ ì‹¤íŒ¨",
                            "_resolved_by": "ai_failure",
                        })
        else:
            print("[Optimized Step 4] AI ë¶„ì„ ë¶ˆí•„ìš” (ëª¨ë“  ë¬¸í•­ í•´ê²°ë¨)")

        # ========================================
        # 4ë‹¨ê³„: ê²°ê³¼ ë³‘í•©
        # ========================================
        print("[Optimized Step 4] ê²°ê³¼ ë³‘í•© ì¤‘...")

        all_resolved = detection_resolved + ai_resolved
        resolved_by_num = {q["question_number"]: q for q in all_resolved}

        # ê¸°ì¡´ ë¬¸í•­ê³¼ ë³‘í•©
        final_questions = []
        for orig_q in existing_questions:
            q_num = orig_q.get("question_number")
            if q_num in resolved_by_num:
                # ì •ì˜¤ë‹µ ì •ë³´ ë³‘í•©
                resolved = resolved_by_num[q_num]
                merged = {**orig_q, **resolved}
                final_questions.append(merged)
            else:
                # í•´ê²°ë˜ì§€ ì•ŠìŒ â†’ null ì²˜ë¦¬
                orig_q["is_correct"] = None
                orig_q["earned_points"] = None
                orig_q["grading_rationale"] = "ë¶„ì„ ì‹¤íŒ¨"
                final_questions.append(orig_q)

        # êµì°¨ ê²€ì¦ (AIë¡œ í•´ê²°ëœ ê²ƒë§Œ)
        ai_result = {"questions": [q for q in final_questions if q.get("_resolved_by") == "ai_analysis"]}
        if ai_result["questions"]:
            ai_result = self._cross_validate_grading(ai_result, marks_result)
            # êµì°¨ ê²€ì¦ ê²°ê³¼ ë°˜ì˜
            for validated_q in ai_result.get("questions", []):
                q_num = validated_q.get("question_number")
                for i, fq in enumerate(final_questions):
                    if fq.get("question_number") == q_num:
                        final_questions[i] = {**fq, **validated_q}
                        break

        # í† í° ì ˆì•½ ì¶”ì •
        # ì „ì²´ AI ë¶„ì„ ì‹œ ì•½ 200í† í°/ë¬¸í•­ â†’ ë¶ˆí™•ì‹¤ ë¬¸í•­ë§Œ ë¶„ì„
        full_ai_tokens = stats["total_questions"] * 200
        actual_ai_tokens = len(uncertain_nums) * 150  # ìµœì í™” í”„ë¡¬í”„íŠ¸ëŠ” ë” ì§§ìŒ
        stats["tokens_saved_estimate"] = max(0, full_ai_tokens - actual_ai_tokens)

        elapsed = time.time() - start_time

        result = {
            "questions": final_questions,
            "_optimization_stats": stats,
            "_cross_validation": ai_result.get("_cross_validation", {}),
        }

        print(f"[Optimized] ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        print(f"  - O/X íƒì§€ë¡œ í•´ê²°: {stats['resolved_by_detection']}ê°œ")
        print(f"  - AI ë¶„ì„: {stats['resolved_by_ai']}ê°œ")
        print(f"  - í† í° ì ˆì•½ (ì¶”ì •): {stats['tokens_saved_estimate']}ê°œ")

        return result

    # ============================================
    # 4. ê¸°ë³¸ ë¶„ì„ (ê¸°ì¡´ í˜¸í™˜)
    # ============================================
    async def analyze_exam_file(
        self,
        file_path: str,
        dynamic_prompt_additions: str = "",
        exam_type: str = "blank",
        custom_prompt: str | None = None,
        subject: str = "ìˆ˜í•™",
    ) -> dict:
        """Analyze exam file (image or PDF) using Gemini.

        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš° ì½¤ë§ˆë¡œ êµ¬ë¶„)
            dynamic_prompt_additions: í•™ìŠµëœ íŒ¨í„´ì—ì„œ ë™ì ìœ¼ë¡œ ì¶”ê°€í•  í”„ë¡¬í”„íŠ¸ ë‚´ìš©
            exam_type: ì‹œí—˜ì§€ ìœ í˜• (blank: ë¹ˆ ì‹œí—˜ì§€, student: í•™ìƒ ë‹µì•ˆì§€)
            custom_prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ (ì§€ì • ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ëŒ€ì²´)
            subject: ê³¼ëª© (ìˆ˜í•™/ì˜ì–´)
        """
        if not self.client:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="AI service is not configured (Missing API Key)."
            )

        # ì—¬ëŸ¬ íŒŒì¼ ê²½ë¡œ íŒŒì‹± (ì½¤ë§ˆ êµ¬ë¶„)
        file_paths = [p.strip() for p in file_path.split(",")]
        file_parts = []

        for fp in file_paths:
            file_content, mime_type = await self._load_file_content(fp)
            if file_content is None:
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {fp}")
            file_parts.append(types.Part.from_bytes(data=file_content, mime_type=mime_type))

        # ì—¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
        multi_page_note = ""
        if len(file_parts) > 1:
            multi_page_note = f"\n\nâš ï¸ ì´ ì‹œí—˜ì§€ëŠ” {len(file_parts)}ê°œì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  í˜ì´ì§€ì˜ ë¬¸ì œë¥¼ ë¹ ì§ì—†ì´ ë¶„ì„í•´ì£¼ì„¸ìš”.\n"

        try:
            # í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if custom_prompt:
                prompt = custom_prompt
            elif exam_type == "student":
                prompt = self._get_student_prompt()
            else:
                prompt = self._get_blank_prompt()

            # ì—¬ëŸ¬ í˜ì´ì§€ ì•ˆë‚´ ë° í•™ìŠµëœ íŒ¨í„´ ì¶”ê°€
            prompt += multi_page_note
            if dynamic_prompt_additions:
                prompt += f"\n\n{dynamic_prompt_additions}"

            # íŒŒì¼ íŒŒíŠ¸ + í”„ë¡¬í”„íŠ¸ íŒŒíŠ¸ ê²°í•©
            all_parts = file_parts + [types.Part.from_text(text=prompt)]

            # Call Gemini with retry logic
            max_retries = 3
            last_error = None
            retry_prompt_addition = ""  # ëˆ„ë½ ê°ì§€ ì‹œ ì¶”ê°€í•  í”„ë¡¬í”„íŠ¸

            for attempt in range(max_retries):
                try:
                    # ì¬ë¶„ì„ ì‹œ ì¶”ê°€ í”„ë¡¬í”„íŠ¸ í¬í•¨
                    current_parts = file_parts + [types.Part.from_text(text=prompt + retry_prompt_addition)]

                    # Gemini API í˜¸ì¶œ (5ë¶„ íƒ€ì„ì•„ì›ƒ)
                    try:
                        response = await asyncio.wait_for(
                            asyncio.to_thread(
                                self.client.models.generate_content,
                                model=self.model_name,
                                contents=[
                                    types.Content(
                                        role="user",
                                        parts=current_parts,
                                    ),
                                ],
                                config=types.GenerateContentConfig(
                                    response_mime_type="application/json",
                                    temperature=0.1,
                                    max_output_tokens=65536,  # Gemini 2.5 Flash max
                                ),
                            ),
                            timeout=300.0  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        )
                    except asyncio.TimeoutError:
                        raise ValueError(
                            "AI ë¶„ì„ì´ 5ë¶„ ì œí•œ ì‹œê°„ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                            "ì‹œí—˜ì§€ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                            "ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë‚®ì¶”ê±°ë‚˜ í˜ì´ì§€ë¥¼ ë‚˜ëˆ ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        )

                    # Check finish reason
                    if response.candidates:
                        candidate = response.candidates[0]
                        finish_reason = getattr(candidate, 'finish_reason', None)
                        print(f"[Attempt {attempt + 1}] Finish reason: {finish_reason}")

                        if finish_reason and "MAX_TOKENS" in str(finish_reason):
                            # 65536 í† í°ì—ì„œë„ ì˜ë¦¬ë©´ ë” ì´ìƒ ì¬ì‹œë„ ë¶ˆê°€
                            raise ValueError(
                                "ì‘ë‹µì´ ìµœëŒ€ í† í° í•œë„(65536)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                                "ì‹œí—˜ì§€ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ë¬¸ì œ ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤."
                            )

                    # Parse JSON
                    if not response.text:
                        raise ValueError("Empty response from AI")

                    result = self._parse_json_response(response.text)

                    # ê²€ì¦ ë° ì‹ ë¢°ë„ ê³„ì‚°
                    validated_result, confidence = self._validate_result(result, exam_type, subject)
                    print(f"[Analysis] Confidence: {confidence:.2f}, Questions: {len(validated_result.get('questions', []))}")

                    # ëˆ„ë½ ê°ì§€ ì‹œ 1íšŒ ì¬ë¶„ì„ ì‹œë„
                    missing_nums = validated_result.get("_missing_questions", [])
                    if missing_nums and attempt == 0:
                        print(f"[Analysis] ëˆ„ë½ ê°ì§€ë¨: {missing_nums}, ì¬ë¶„ì„ ì‹œë„...")
                        retry_prompt_addition = f"""

âš ï¸ **ì¬ë¶„ì„ ìš”ì²­** - ë‹¤ìŒ ë¬¸í•­ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_nums}

ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ì¶”ë¡ í•´ì„œ **ë°˜ë“œì‹œ** ì´ ë²ˆí˜¸ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
- 1ë²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ 2ë²ˆ
- Në²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ N+1ë²ˆ
- ë²ˆí˜¸ê°€ ê°€ë ¤ì ¸ë„ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ ë¶€ì—¬

ëˆ„ë½ ì—†ì´ ë‹¤ì‹œ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
                        continue  # ë‹¤ìŒ attemptë¡œ ì¬ì‹œë„ (retry_prompt_addition í¬í•¨ë¨)

                    # ì¬ë¶„ì„ í›„ì—ë„ ëˆ„ë½ëœ ë¬¸í•­ì´ ìˆìœ¼ë©´ placeholder ì¶”ê°€
                    final_missing = validated_result.get("_missing_questions", [])
                    if final_missing:
                        print(f"[Analysis] ì¬ë¶„ì„ í›„ì—ë„ ëˆ„ë½: {final_missing}, placeholder ì¶”ê°€")
                        questions = validated_result.get("questions", [])

                        # ê¸°ì¡´ ë¬¸í•­ë“¤ì˜ í‰ê·  ë°°ì  ê³„ì‚° (placeholder ê¸°ë³¸ê°’ìš©)
                        existing_points = [q.get("points", 4) for q in questions if q.get("points")]
                        avg_points = round(sum(existing_points) / len(existing_points), 1) if existing_points else 4.0

                        for num in final_missing:
                            placeholder_q = {
                                "question_number": num,
                                "difficulty": "pattern",  # ê¸°ë³¸ê°’
                                "question_type": "calculation",  # ê¸°ë³¸ê°’
                                "question_format": "objective",  # ê¸°ë³¸ê°’
                                "topic": "ë¶„ì„ ì‹¤íŒ¨ > ìˆ˜ë™ í™•ì¸ í•„ìš”",
                                "points": avg_points,
                                "confidence": 0.3,  # ë‚®ì€ ì‹ ë¢°ë„
                                "confidence_reason": "AIê°€ ì´ ë¬¸í•­ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                                "ai_comment": "âš ï¸ ì´ ë¬¸í•­ì€ AIê°€ ì¸ì‹í•˜ì§€ ëª»í•´ ìë™ ìƒì„±ëœ placeholderì…ë‹ˆë‹¤.",
                                "_is_placeholder": True,
                            }
                            questions.append(placeholder_q)

                        # ë¬¸í•­ ë²ˆí˜¸ìˆœ ì •ë ¬
                        questions.sort(key=lambda q: (
                            int(q["question_number"]) if str(q.get("question_number", "")).isdigit()
                            else float('inf')
                        ))
                        validated_result["questions"] = questions
                        print(f"[Analysis] Placeholder ì¶”ê°€ ì™„ë£Œ. ì´ {len(questions)}ê°œ ë¬¸í•­")

                    return validated_result

                except json.JSONDecodeError as e:
                    last_error = e
                    print(f"[Attempt {attempt + 1}] JSON parse error: {e}")
                    print(f"Response text (first 500 chars): {response.text[:500] if response.text else 'None'}")
                    continue
                except Exception as e:
                    last_error = e
                    print(f"[Attempt {attempt + 1}] Error: {e}")
                    continue

            # All retries failed
            raise ValueError(f"Failed after {max_retries} attempts. Last error: {last_error}")

        except Exception as e:
            print(f"AI Analysis Error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"AI Analysis failed: {str(e)}"
            )

    def _validate_result(self, result: dict, exam_type: str = "blank", subject: str = "ìˆ˜í•™") -> tuple[dict, float]:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦ ë° ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°.

        Args:
            result: AI ë¶„ì„ ê²°ê³¼
            exam_type: ì‹œí—˜ì§€ ìœ í˜• (blank/student/unified)
            subject: ê³¼ëª© (ìˆ˜í•™/ì˜ì–´)
        """
        confidence = 1.0
        issues = []

        # unified ëª¨ë“œ: paper_typeì—ì„œ ì‹¤ì œ ìœ í˜• ì¶”ë¡ 
        if exam_type == "unified":
            paper_type = result.get("paper_type", "blank")
            if paper_type in ["answered", "mixed"]:
                exam_type = "student"
            else:
                exam_type = "blank"

        # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if "summary" not in result:
            result["summary"] = self._empty_summary()
            confidence -= 0.3
            issues.append("summary ëˆ„ë½")

        if "questions" not in result or not result["questions"]:
            confidence -= 0.5
            issues.append("questions ëˆ„ë½")

        # 2. ë¬¸í•­ë³„ ê²€ì¦
        # 4ë‹¨ê³„ ì‹œìŠ¤í…œ (ì‹ ê·œ) + 3ë‹¨ê³„ ì‹œìŠ¤í…œ (í•˜ìœ„ í˜¸í™˜)
        valid_difficulties = {"concept", "pattern", "reasoning", "creative", "high", "medium", "low"}
        # ê³¼ëª©ë³„ ìœ íš¨ question_type ë™ì  ë¡œë“œ
        valid_types = get_valid_question_types(subject)
        # ê³¼ëª©ë³„ ê¸°ë³¸ question_type
        default_question_type = "calculation" if subject == "ìˆ˜í•™" else "grammar"

        # 3ë‹¨ê³„ â†’ 4ë‹¨ê³„ ë³€í™˜ ë§¤í•‘
        difficulty_conversion = {
            "low": "concept",      # í•˜ â†’ ê°œë…
            "medium": "pattern",   # ì¤‘ â†’ ìœ í˜•
            "high": "reasoning",   # ìƒ â†’ ì‹¬í™”
        }

        for i, q in enumerate(result.get("questions", [])):
            q_confidence = q.get("confidence", 0.9)

            # ë‚œì´ë„ ê²€ì¦
            if q.get("difficulty") not in valid_difficulties:
                q["difficulty"] = "pattern"
                confidence -= 0.05
                q_confidence -= 0.15
                issues.append(f"Q{i+1}: ì˜ëª»ëœ ë‚œì´ë„")
            # 3ë‹¨ê³„ â†’ 4ë‹¨ê³„ ìë™ ë³€í™˜
            elif q.get("difficulty") in difficulty_conversion:
                q["difficulty"] = difficulty_conversion[q["difficulty"]]

            # ìœ í˜• ê²€ì¦
            if q.get("question_type") not in valid_types:
                q["question_type"] = default_question_type
                confidence -= 0.05
                q_confidence -= 0.15
                issues.append(f"Q{i+1}: ì˜ëª»ëœ ìœ í˜•")

            # í† í”½ í˜•ì‹ ê²€ì¦ ë° ì •ê·œí™”
            topic = q.get("topic", "")
            if topic:
                # ë³µí•© í† í”½ ì²˜ë¦¬: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
                # ì˜ˆ: "ë„í˜•ì˜ ë°©ì •ì‹ > ì§ì„ ì˜ ë°©ì •ì‹, ì›ì˜ ë°©ì •ì‹" â†’ "ë„í˜•ì˜ ë°©ì •ì‹ > ì§ì„ ì˜ ë°©ì •ì‹"
                if "," in topic:
                    parts = topic.split(" > ")
                    if len(parts) >= 3:
                        # ë§ˆì§€ë§‰ íŒŒíŠ¸(ì†Œë‹¨ì›)ì—ì„œ ì½¤ë§ˆ ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        last_part = parts[-1].split(",")[0].strip()
                        q["topic"] = " > ".join(parts[:-1] + [last_part])
                        issues.append(f"Q{i+1}: ë³µí•© í† í”½ ì •ê·œí™”")
                    topic = q["topic"]

                # "Aì™€ B", "A ë° B" íŒ¨í„´ ì²˜ë¦¬
                if " > " in topic:
                    parts = topic.split(" > ")
                    last_part = parts[-1]
                    for separator in ["ì™€ ", "ê³¼ ", " ë° ", ", "]:
                        if separator in last_part:
                            last_part = last_part.split(separator)[0].strip()
                            q["topic"] = " > ".join(parts[:-1] + [last_part])
                            issues.append(f"Q{i+1}: ë³µí•© í† í”½ ì •ê·œí™”")
                            break

                # í˜•ì‹ ê²€ì¦
                if " > " not in q.get("topic", ""):
                    confidence -= 0.03
                    q_confidence -= 0.1
                    issues.append(f"Q{i+1}: í† í”½ í˜•ì‹ ì˜¤ë¥˜")

            # ë°°ì  ê²€ì¦ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ë¥˜ ë°©ì§€: ì†Œìˆ˜ì  1ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)
            points = q.get("points")
            if points is None or points <= 0:
                q["points"] = 4
                confidence -= 0.02
                q_confidence -= 0.05
            else:
                # Geminiê°€ 3.9999999 ê°™ì€ ê°’ì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ê·œí™”
                q["points"] = round(points, 1)

            # í•™ìƒ ë‹µì•ˆì§€ìš© í•„ë“œ ê²€ì¦
            if exam_type == "student":
                # ê³¼ëª©ë³„ ìœ íš¨ error_type ë™ì  ë¡œë“œ
                valid_error_types = get_valid_error_types(subject) | {None}
                # ê³¼ëª©ë³„ ê¸°ë³¸ error_type
                default_error_type = "concept_error" if subject == "ìˆ˜í•™" else "comprehension_error"
                error_type = q.get("error_type")
                if error_type and error_type not in valid_error_types:
                    q["error_type"] = default_error_type
                    q_confidence -= 0.05

                if "is_correct" not in q:
                    q["is_correct"] = None

                if "earned_points" not in q:
                    if q.get("is_correct") is True:
                        q["earned_points"] = q.get("points", 0)
                    elif q.get("is_correct") is False:
                        q["earned_points"] = 0
                    else:
                        q["earned_points"] = None
                elif q.get("earned_points") is not None:
                    # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ë¥˜ ë°©ì§€: ì†Œìˆ˜ì  1ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                    q["earned_points"] = round(q["earned_points"], 1)

            q["confidence"] = round(max(0.0, min(1.0, q_confidence)), 2)

        # 3. ë¬¸í•­ ë²ˆí˜¸ ì—°ì†ì„± ê²€ì¦ (ëˆ„ë½ ê°ì§€)
        if result.get("questions"):
            question_numbers = []
            for q in result["questions"]:
                qnum = q.get("question_number")
                if isinstance(qnum, int):
                    question_numbers.append(qnum)
                elif isinstance(qnum, str) and qnum.isdigit():
                    question_numbers.append(int(qnum))

            if question_numbers:
                question_numbers.sort()
                expected_nums = list(range(1, max(question_numbers) + 1))
                missing_nums = set(expected_nums) - set(question_numbers)

                if missing_nums:
                    confidence -= 0.1 * len(missing_nums)
                    issues.append(f"ëˆ„ë½ëœ ë¬¸í•­: {sorted(missing_nums)}")
                    result["_missing_questions"] = sorted(missing_nums)
                    print(f"[Validation] âš ï¸ ëˆ„ë½ëœ ë¬¸í•­ ë²ˆí˜¸: {sorted(missing_nums)}")

        # 3.5. ì´ì  100ì  ê²€ì¦
        if result.get("questions"):
            total_points = sum(q.get("points", 0) for q in result["questions"])
            result["_total_points"] = total_points

            # ì´ì ì´ 100ì ì´ ì•„ë‹ˆë©´ ì‹ ë¢°ë„ ê°ì†Œ
            if total_points != 100:
                # ì˜¤ì°¨ ë²”ìœ„ì— ë”°ë¼ ì‹ ë¢°ë„ ê°ì†Œ
                point_diff = abs(100 - total_points)
                if point_diff <= 2:
                    # 2ì  ì´ë‚´ ì˜¤ì°¨: ê²½ë¯¸í•œ ê°ì 
                    confidence -= 0.05
                    issues.append(f"ì´ì  ì˜¤ì°¨: {total_points}ì  (100ì  ê¸°ì¤€)")
                elif point_diff <= 10:
                    # 10ì  ì´ë‚´ ì˜¤ì°¨: ì¤‘ê°„ ê°ì 
                    confidence -= 0.15
                    issues.append(f"ì´ì  ë¶ˆì¼ì¹˜: {total_points}ì  (100ì  ê¸°ì¤€)")
                else:
                    # 10ì  ì´ˆê³¼ ì˜¤ì°¨: í° ê°ì 
                    confidence -= 0.25
                    issues.append(f"ì´ì  í¬ê²Œ ë¶ˆì¼ì¹˜: {total_points}ì  (100ì  ê¸°ì¤€)")
                print(f"[Validation] âš ï¸ ì´ì  ë¶ˆì¼ì¹˜: {total_points}ì  (100ì  ê¸°ì¤€, ì˜¤ì°¨ {point_diff}ì )")

        # 4. ë¶„í¬ ì¼ì¹˜ ê²€ì¦
        if result.get("questions"):
            # 4ë‹¨ê³„ ì‹œìŠ¤í…œë§Œ ì‚¬ìš© (3ë‹¨ê³„ëŠ” ìœ„ì—ì„œ ë³€í™˜ë¨)
            actual_diff = {
                "concept": 0, "pattern": 0, "reasoning": 0, "creative": 0
            }
            actual_type: dict[str, int] = {}

            for q in result["questions"]:
                diff = q.get("difficulty", "pattern")  # ê¸°ë³¸ê°’ì„ patternìœ¼ë¡œ ë³€ê²½
                if diff in actual_diff:
                    actual_diff[diff] += 1

                qtype = q.get("question_type", "calculation")
                actual_type[qtype] = actual_type.get(qtype, 0) + 1

            result["summary"]["difficulty_distribution"] = actual_diff
            result["summary"]["type_distribution"] = {
                "calculation": actual_type.get("calculation", 0),
                "geometry": actual_type.get("geometry", 0),
                "application": actual_type.get("application", 0),
                "proof": actual_type.get("proof", 0),
                "graph": actual_type.get("graph", 0),
                "statistics": actual_type.get("statistics", 0),
            }

        # 5. êµì‚¬ ê²€í†  í•„ìš” ì—¬ë¶€ íŒì •
        low_confidence_count = sum(
            1 for q in result.get("questions", [])
            if q.get("confidence", 1.0) < self.grading_confidence_threshold
        )
        review_needed = result.get("requires_human_review", False)
        review_reasons = []

        if low_confidence_count >= 2:
            review_needed = True
            review_reasons.append(f"ì €ì‹ ë¢°ë„ ë¬¸í•­ {low_confidence_count}ê°œ")

        # ê°œë³„ ë¬¸í•­ ì¤‘ requires_reviewê°€ ìˆëŠ” ê²½ìš°
        review_questions = [
            q.get("question_number") for q in result.get("questions", [])
            if q.get("requires_review", False)
        ]
        if review_questions:
            review_needed = True
            review_reasons.append(f"ê²€í†  í•„ìš” ë¬¸í•­: {review_questions}")

        # ëˆ„ë½ëœ ë¬¸í•­ì´ ìˆëŠ” ê²½ìš°
        if result.get("_missing_questions"):
            review_needed = True
            review_reasons.append(f"ëˆ„ë½ ë¬¸í•­: {result['_missing_questions']}")

        result["requires_human_review"] = review_needed
        if review_reasons:
            result["review_reason"] = ", ".join(review_reasons)

        # 6. ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜
        confidence = max(0.0, min(1.0, confidence))

        if issues:
            print(f"[Validation] Issues found: {issues}")
            print(f"[Validation] Confidence: {confidence:.2f}")

        if review_needed:
            print(f"[Validation] âš ï¸ êµì‚¬ ê²€í†  í•„ìš”: {result.get('review_reason')}")

        result["_confidence"] = round(confidence, 2)
        result["_validation_issues"] = issues

        return result, confidence

    def _validate_and_fix_category(self, result: dict, category: str | None) -> dict:
        """ë¶„ì„ ê²°ê³¼ì˜ topicì—ì„œ ê³¼ëª©ëª…(category)ì„ ê²€ì¦í•˜ê³  ìë™ ìˆ˜ì •.

        Args:
            result: AI ë¶„ì„ ê²°ê³¼
            category: ì„ íƒí•œ ì„¸ë¶€ ê³¼ëª© (ì˜ˆ: "ê³µí†µìˆ˜í•™2", "ëŒ€ìˆ˜")

        Returns:
            ìˆ˜ì •ëœ ë¶„ì„ ê²°ê³¼
        """
        if not category:
            return result

        questions = result.get("questions", [])
        if not questions:
            return result

        fixed_count = 0
        mismatched_topics = []

        # category ì •ê·œí™” (ê³µë°± ì œê±°, ìˆ«ì í†µì¼)
        normalized_category = category.strip()
        # "ê³µí†µìˆ˜í•™1", "ê³µí†µìˆ˜í•™2" ë“± íŒ¨í„´ ë§¤ì¹­ìš©
        category_patterns = [
            normalized_category,
            normalized_category.replace("ìˆ˜í•™", " ìˆ˜í•™"),  # "ê³µí†µ ìˆ˜í•™2" í˜•íƒœ
        ]

        for q in questions:
            topic = q.get("topic", "")
            if not topic or " > " not in topic:
                continue

            # topicì—ì„œ ê³¼ëª©ëª… ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¶€ë¶„)
            parts = topic.split(" > ")
            current_subject = parts[0].strip()

            # í˜„ì¬ topicì˜ ê³¼ëª©ëª…ì´ ì„ íƒí•œ categoryì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            is_match = any(
                pattern in current_subject or current_subject in pattern
                for pattern in category_patterns
            )

            if not is_match:
                # ë¶ˆì¼ì¹˜: topicì˜ ê³¼ëª©ëª…ì„ categoryë¡œ ë³€ê²½
                old_topic = topic
                new_parts = [normalized_category] + parts[1:]  # ê³¼ëª©ëª…ë§Œ êµì²´
                q["topic"] = " > ".join(new_parts)
                fixed_count += 1
                mismatched_topics.append({
                    "question_number": q.get("question_number"),
                    "old_topic": old_topic,
                    "new_topic": q["topic"],
                })

        # ìˆ˜ì • ê²°ê³¼ ì €ì¥
        if fixed_count > 0:
            result["_category_fix_count"] = fixed_count
            result["_category_fixes"] = mismatched_topics
            print(f"[Category Validation] âš ï¸ {fixed_count}ê°œ ë¬¸í•­ì˜ ê³¼ëª©ëª…ì„ '{normalized_category}'ë¡œ ìë™ ìˆ˜ì •")
            for fix in mismatched_topics[:3]:  # ìµœëŒ€ 3ê°œë§Œ ë¡œê·¸
                print(f"  - Q{fix['question_number']}: {fix['old_topic']} â†’ {fix['new_topic']}")
            if fixed_count > 3:
                print(f"  ... ì™¸ {fixed_count - 3}ê°œ ë¬¸í•­")

        return result

    def _empty_summary(self) -> dict:
        """ë¹ˆ summary ìƒì„±."""
        return {
            "difficulty_distribution": {
                "concept": 0, "pattern": 0, "reasoning": 0, "creative": 0
            },
            "type_distribution": {
                "calculation": 0, "geometry": 0, "application": 0,
                "proof": 0, "graph": 0, "statistics": 0
            },
            "average_difficulty": "pattern",
            "dominant_type": "calculation"
        }

    def _parse_json_response(self, text: str) -> dict:
        """Gemini ì‘ë‹µì—ì„œ JSON íŒŒì‹± (í›„í–‰ ì‰¼í‘œ ë“± ì •ë¦¬)."""
        import re

        json_text = text.strip()

        # ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
        if json_text.startswith("```"):
            json_text = json_text.split("\n", 1)[1] if "\n" in json_text else json_text[3:]
        if json_text.endswith("```"):
            json_text = json_text[:-3]
        json_text = json_text.strip()

        try:
            return json.loads(json_text)
        except json.JSONDecodeError:
            # í›„í–‰ ì‰¼í‘œ ì œê±° ì‹œë„
            cleaned = re.sub(r',(\s*[}\]])', r'\1', json_text)
            return json.loads(cleaned)

    def _get_mime_type(self, file_path: Path) -> str:
        """íŒŒì¼ í™•ì¥ìë¡œ MIME íƒ€ì… ê²°ì •."""
        suffix = file_path.suffix.lower()
        if suffix == ".png":
            return "image/png"
        elif suffix == ".pdf":
            return "application/pdf"
        elif suffix in [".jpg", ".jpeg"]:
            return "image/jpeg"
        else:
            return "image/jpeg"

    def _get_mime_type_from_path(self, file_path: str) -> str:
        """íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´ì—ì„œ MIME íƒ€ì… ê²°ì •."""
        path_lower = file_path.lower()
        if path_lower.endswith(".png"):
            return "image/png"
        elif path_lower.endswith(".pdf"):
            return "application/pdf"
        elif path_lower.endswith(".jpg") or path_lower.endswith(".jpeg"):
            return "image/jpeg"
        else:
            return "image/jpeg"

    async def _load_file_content(self, file_path: str) -> tuple[bytes | None, str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì½˜í…ì¸ ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ë¡œì»¬ íŒŒì¼ ë˜ëŠ” Supabase Storageì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ ë˜ëŠ” supabase://...)

        Returns:
            (file_content, mime_type) íŠœí”Œ
        """
        from app.services.file_storage import file_storage

        if file_path.startswith("supabase://"):
            # Supabase Storageì—ì„œ ë‹¤ìš´ë¡œë“œ
            try:
                content = await file_storage.download_file(file_path)
                mime_type = self._get_mime_type_from_path(file_path)
                print(f"[FileLoad] Downloaded from Supabase: {file_path[:50]}... ({len(content)} bytes)")
                return content, mime_type
            except Exception as e:
                print(f"[FileLoad] Supabase download failed: {e}")
                return None, ""
        else:
            # ë¡œì»¬ íŒŒì¼
            path = Path(file_path)
            if not path.exists():
                print(f"[FileLoad] Local file not found: {file_path}")
                return None, ""
            content = path.read_bytes()
            mime_type = self._get_mime_type(path)
            print(f"[FileLoad] Read local file: {file_path} ({len(content)} bytes)")
            return content, mime_type

    def _get_blank_prompt(self) -> str:
        """ë¹ˆ ì‹œí—˜ì§€ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return """
ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™êµ ìˆ˜í•™ ì‹œí—˜ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ë¶„ì„ ë‹¨ê³„ (Chain of Thought)

### STEP 1: ë¬¸ì œ ì¶”ì¶œ (âš ï¸ ëˆ„ë½ ê¸ˆì§€)

ğŸ¯ **í•µì‹¬ ê·œì¹™: ë°°ì  í‘œì‹œê°€ ìˆëŠ” ê³³ = ë¬¸í•­ì´ ìˆëŠ” ê³³**

ì‹œí—˜ì§€ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³ :

**1ë‹¨ê³„: ë°°ì  ì°¾ê¸° ([Nì ] í˜•ì‹)**
- [6ì ], [8ì ], [9ì ] ë“±ì˜ ë°°ì  í‘œì‹œë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
- ë°°ì ì´ ìˆëŠ” ê³³ë§ˆë‹¤ ë°˜ë“œì‹œ ë¬¸í•­ì´ ìˆìŠµë‹ˆë‹¤
- ì˜ˆì‹œ:
  ```
  1. ë¬¸ì œë‚´ìš©... [6ì ]  â† 1ë²ˆ ë¬¸í•­
  2. ë¬¸ì œë‚´ìš©... [6ì ]  â† 2ë²ˆ ë¬¸í•­
  3. ë¬¸ì œë‚´ìš©... [8ì ]  â† 3ë²ˆ ë¬¸í•­
  4. ë¬¸ì œë‚´ìš©... [9ì ]  â† 4ë²ˆ ë¬¸í•­
  ```

**2ë‹¨ê³„: ë¬¸í•­ ë²ˆí˜¸ í™•ì¸**
- ì´ ë¬¸í•­ ìˆ˜ = ë°°ì  í‘œì‹œ ê°œìˆ˜
- ë¬¸í•­ ë²ˆí˜¸ê°€ ì—°ì†ì ì¸ì§€ í™•ì¸ (1, 2, 3, 4, ...)
- ì„œë‹µí˜• ë¬¸ì œë„ ë°°ì ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨

âš ï¸ **í•„ìˆ˜**: 1ë²ˆë¶€í„° ë§ˆì§€ë§‰ ë¬¸í•­ê¹Œì§€ **ë¹ ì§ì—†ì´** ëª¨ë‘ ë¶„ì„í•˜ì„¸ìš”.
- ì±„ì  í‘œì‹œ(X, O, âœ“)ê°€ í¬ê²Œ ìˆì–´ë„ í•´ë‹¹ ë¬¸í•­ì„ ë°˜ë“œì‹œ í¬í•¨
- ì†ê¸€ì”¨ë‚˜ ë¹¨ê°„íœì´ ë§ì•„ë„ ë°°ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸í•­ ì¸ì‹
- ë¬¸í•­ ë²ˆí˜¸ê°€ ê°€ë ¤ì ¸ë„ **ë°°ì  ê°œìˆ˜**ë¡œ ì´ ë¬¸í•­ ìˆ˜ íŒŒì•…

ğŸ”¢ **ë²ˆí˜¸ ì¶”ë¡  ê·œì¹™**:
- ë²ˆí˜¸ê°€ ê°€ë ¤ì§€ê±°ë‚˜ ì•ˆ ë³´ì—¬ë„, **ìœ„ì¹˜ì™€ ìˆœì„œë¡œ ë²ˆí˜¸ë¥¼ ì¶”ë¡ **í•˜ì„¸ìš”
- 1ë²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ 2ë²ˆ
- Në²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ N+1ë²ˆ
- í° X í‘œì‹œë‚˜ ì±„ì  ë§ˆí¬ë¡œ ë²ˆí˜¸ê°€ ê°€ë ¤ì ¸ë„ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ ë¶€ì—¬

### STEP 2: ë¬¸í•­ë³„ ë¶„ë¥˜
ê° ë¬¸í•­ì— ëŒ€í•´:
1. ì–´ë–¤ ê°œë…ì„ ë¬»ëŠ”ê°€? â†’ í† í”½ ë¶„ë¥˜
2. ì–¼ë§ˆë‚˜ ì–´ë ¤ìš´ê°€? â†’ ë‚œì´ë„ íŒì •
3. ì–´ë–¤ ìœ í˜•ì¸ê°€? â†’ ë¬¸ì œ ìœ í˜•

### STEP 3: JSON ì¶œë ¥
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì¶œë ¥í•˜ì„¸ìš”:

{
    "exam_info": {
        "total_questions": 16,
        "total_points": 100,
        "objective_count": 12,
        "subjective_count": 4
    },
    "summary": {
        "difficulty_distribution": {"concept": 0, "pattern": 0, "reasoning": 0, "creative": 0},
        "type_distribution": {
            "calculation": 0, "geometry": 0, "application": 0,
            "proof": 0, "graph": 0, "statistics": 0
        },
        "average_difficulty": "pattern",
        "dominant_type": "calculation"
    },
    "questions": [
        {
            "question_number": 1,
            "difficulty": "concept",
            "question_type": "calculation",
            "points": 3,
            "topic": "ê³µí†µìˆ˜í•™1 > ë‹¤í•­ì‹ > ë‹¤í•­ì‹ì˜ ì—°ì‚°",
            "ai_comment": "í•µì‹¬ ê°œë…. ì£¼ì˜ì‚¬í•­.",
            "confidence": 0.95,
            "difficulty_reason": "ê¸°ë³¸ ê°œë… í™•ì¸"
        }
    ]
}

## í† í”½ ë¶„ë¥˜í‘œ (2022 ê°œì • êµìœ¡ê³¼ì • - ì •í™•íˆ ì‚¬ìš©)

### ã€ì´ˆë“±í•™êµã€‘
[ì´ˆ1-1] 9ê¹Œì§€ì˜ ìˆ˜, ì—¬ëŸ¬ ê°€ì§€ ëª¨ì–‘, ë§ì…ˆê³¼ ëº„ì…ˆ(1), ë¹„êµí•˜ê¸°, 50ê¹Œì§€ì˜ ìˆ˜
[ì´ˆ1-2] 100ê¹Œì§€ì˜ ìˆ˜, ë§ì…ˆê³¼ ëº„ì…ˆ(2), ì—¬ëŸ¬ ê°€ì§€ ëª¨ì–‘, ë§ì…ˆê³¼ ëº„ì…ˆ(3), ì‹œê³„ ë³´ê¸°ì™€ ê·œì¹™ ì°¾ê¸°
[ì´ˆ2-1] ì„¸ ìë¦¬ ìˆ˜, ì—¬ëŸ¬ ê°€ì§€ ë„í˜•, ë§ì…ˆê³¼ ëº„ì…ˆ, ê¸¸ì´ ì¬ê¸°, ë¶„ë¥˜í•˜ê¸°, ê³±ì…ˆ
[ì´ˆ2-2] ë„¤ ìë¦¬ ìˆ˜, ê³±ì…ˆêµ¬êµ¬, ê¸¸ì´ ì¬ê¸°, ì‹œê°ê³¼ ì‹œê°„, í‘œì™€ ê·¸ë˜í”„, ê·œì¹™ ì°¾ê¸°
[ì´ˆ3-1] ë§ì…ˆê³¼ ëº„ì…ˆ, í‰ë©´ë„í˜•, ë‚˜ëˆ—ì…ˆ, ê³±ì…ˆ, ê¸¸ì´ì™€ ì‹œê°„, ë¶„ìˆ˜ì™€ ì†Œìˆ˜
[ì´ˆ3-2] ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ, ì›, ë¶„ìˆ˜, ë“¤ì´ì™€ ë¬´ê²Œ, ìë£Œì˜ ì •ë¦¬
[ì´ˆ4-1] í° ìˆ˜, ê°ë„, ê³±ì…ˆê³¼ ë‚˜ëˆ—ì…ˆ, í‰ë©´ë„í˜•ì˜ ì´ë™, ë§‰ëŒ€ê·¸ë˜í”„, ê·œì¹™ ì°¾ê¸°
[ì´ˆ4-2] ë¶„ìˆ˜ì˜ ë§ì…ˆê³¼ ëº„ì…ˆ, ì‚¼ê°í˜•, ì†Œìˆ˜ì˜ ë§ì…ˆê³¼ ëº„ì…ˆ, ì‚¬ê°í˜•, êº¾ì€ì„ ê·¸ë˜í”„, ë‹¤ê°í˜•
[ì´ˆ5-1] ìì—°ìˆ˜ì˜ í˜¼í•© ê³„ì‚°, ì•½ìˆ˜ì™€ ë°°ìˆ˜, ê·œì¹™ê³¼ ëŒ€ì‘, ì•½ë¶„ê³¼ í†µë¶„, ë¶„ìˆ˜ì˜ ë§ì…ˆê³¼ ëº„ì…ˆ, ë‹¤ê°í˜•ì˜ ë‘˜ë ˆì™€ ë„“ì´
[ì´ˆ5-2] ìˆ˜ì˜ ë²”ìœ„ì™€ ì–´ë¦¼í•˜ê¸°, ë¶„ìˆ˜ì˜ ê³±ì…ˆ, í•©ë™ê³¼ ëŒ€ì¹­, ì†Œìˆ˜ì˜ ê³±ì…ˆ, ì§ìœ¡ë©´ì²´, í‰ê· ê³¼ ê°€ëŠ¥ì„±
[ì´ˆ6-1] ë¶„ìˆ˜ì˜ ë‚˜ëˆ—ì…ˆ, ê°ê¸°ë‘¥ê³¼ ê°ë¿”, ì†Œìˆ˜ì˜ ë‚˜ëˆ—ì…ˆ, ë¹„ì™€ ë¹„ìœ¨, ì—¬ëŸ¬ ê°€ì§€ ê·¸ë˜í”„, ì§ìœ¡ë©´ì²´ì˜ ë¶€í”¼ì™€ ê²‰ë„“ì´
[ì´ˆ6-2] ë¶„ìˆ˜ì˜ ë‚˜ëˆ—ì…ˆ, ì†Œìˆ˜ì˜ ë‚˜ëˆ—ì…ˆ, ê³µê°„ê³¼ ì…ì²´, ë¹„ë¡€ì‹ê³¼ ë¹„ë¡€ë°°ë¶„, ì›ì˜ ë„“ì´, ì›ê¸°ë‘¥/ì›ë¿”/êµ¬

### ã€ì¤‘í•™êµã€‘
[ì¤‘1-1] ì†Œì¸ìˆ˜ë¶„í•´, ì •ìˆ˜ì™€ ìœ ë¦¬ìˆ˜, ë¬¸ìì™€ ì‹, ì¼ì°¨ë°©ì •ì‹, ì¢Œí‘œí‰ë©´ê³¼ ê·¸ë˜í”„
[ì¤‘1-2] ê¸°ë³¸ ë„í˜•, ì‘ë„ì™€ í•©ë™, í‰ë©´ë„í˜•, ì…ì²´ë„í˜•, ìë£Œì˜ ì •ë¦¬ì™€ í•´ì„
[ì¤‘2-1] ìœ ë¦¬ìˆ˜ì™€ ìˆœí™˜ì†Œìˆ˜, ì‹ì˜ ê³„ì‚°, ì¼ì°¨ë¶€ë“±ì‹, ì—°ë¦½ì¼ì°¨ë°©ì •ì‹, ì¼ì°¨í•¨ìˆ˜
[ì¤‘2-2] ì‚¼ê°í˜•ì˜ ì„±ì§ˆ, ì‚¬ê°í˜•ì˜ ì„±ì§ˆ, ë„í˜•ì˜ ë‹®ìŒ, í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬, í™•ë¥ 
[ì¤‘3-1] ì‹¤ìˆ˜ì™€ ê·¸ ì—°ì‚°, ë‹¤í•­ì‹ì˜ ê³±ì…ˆê³¼ ì¸ìˆ˜ë¶„í•´, ì´ì°¨ë°©ì •ì‹, ì´ì°¨í•¨ìˆ˜
[ì¤‘3-2] ì‚¼ê°ë¹„, ì›ì˜ ì„±ì§ˆ, í†µê³„

### ã€ê³ ë“±í•™êµ - ê³µí†µ ê³¼ëª©ã€‘
[ê³µí†µìˆ˜í•™1]
- ë‹¤í•­ì‹: ë‹¤í•­ì‹ì˜ ì—°ì‚°, í•­ë“±ì‹ê³¼ ë‚˜ë¨¸ì§€ì •ë¦¬, ì¸ìˆ˜ë¶„í•´
- ë°©ì •ì‹ê³¼ ë¶€ë“±ì‹: ë³µì†Œìˆ˜, ì´ì°¨ë°©ì •ì‹, ì´ì°¨ë°©ì •ì‹ê³¼ ì´ì°¨í•¨ìˆ˜, ì—¬ëŸ¬ ê°€ì§€ ë°©ì •ì‹, ì—¬ëŸ¬ ê°€ì§€ ë¶€ë“±ì‹
- ê²½ìš°ì˜ ìˆ˜: ê²½ìš°ì˜ ìˆ˜ì™€ ìˆœì—´, ì¡°í•©
- í–‰ë ¬: í–‰ë ¬ì˜ ëœ», í–‰ë ¬ì˜ ì—°ì‚°

[ê³µí†µìˆ˜í•™2]
- ë„í˜•ì˜ ë°©ì •ì‹: í‰ë©´ì¢Œí‘œ, ì§ì„ ì˜ ë°©ì •ì‹, ì›ì˜ ë°©ì •ì‹, ë„í˜•ì˜ ì´ë™
- ì§‘í•©ê³¼ ëª…ì œ: ì§‘í•©ì˜ ëœ», ì§‘í•©ì˜ ì—°ì‚°, ëª…ì œ
- í•¨ìˆ˜ì™€ ê·¸ë˜í”„: í•©ì„±í•¨ìˆ˜ì™€ ì—­í•¨ìˆ˜, ìœ ë¦¬í•¨ìˆ˜, ë¬´ë¦¬í•¨ìˆ˜

### ã€ê³ ë“±í•™êµ - ì¼ë°˜ ì„ íƒ ê³¼ëª©ã€‘
[ëŒ€ìˆ˜]
- ì§€ìˆ˜í•¨ìˆ˜ì™€ ë¡œê·¸í•¨ìˆ˜: ì§€ìˆ˜, ë¡œê·¸, ì§€ìˆ˜í•¨ìˆ˜, ë¡œê·¸í•¨ìˆ˜
- ì‚¼ê°í•¨ìˆ˜: ì‚¼ê°í•¨ìˆ˜ì˜ ì •ì˜, ì‚¼ê°í•¨ìˆ˜ì˜ ê·¸ë˜í”„, ì‚¬ì¸ë²•ì¹™ê³¼ ì½”ì‚¬ì¸ë²•ì¹™
- ìˆ˜ì—´: ë“±ì°¨ìˆ˜ì—´ê³¼ ë“±ë¹„ìˆ˜ì—´, ìˆ˜ì—´ì˜ í•©, ìˆ˜í•™ì  ê·€ë‚©ë²•

[ë¯¸ì ë¶„I]
- í•¨ìˆ˜ì˜ ê·¹í•œê³¼ ì—°ì†: í•¨ìˆ˜ì˜ ê·¹í•œ, í•¨ìˆ˜ì˜ ì—°ì†
- ë¯¸ë¶„: ë¯¸ë¶„ê³„ìˆ˜ì™€ ë„í•¨ìˆ˜, ë„í•¨ìˆ˜ì˜ í™œìš©
- ì ë¶„: ë¶€ì •ì ë¶„, ì •ì ë¶„, ì •ì ë¶„ì˜ í™œìš©

[í™•ë¥ ê³¼ í†µê³„]
- ìˆœì—´ê³¼ ì¡°í•©: ìˆœì—´, ì¡°í•©, ì´í•­ì •ë¦¬
- í™•ë¥ : í™•ë¥ ì˜ ëœ»ê³¼ í™œìš©, ì¡°ê±´ë¶€ í™•ë¥ 
- í†µê³„: í™•ë¥ ë¶„í¬, í†µê³„ì  ì¶”ì •, ëª¨ë¹„ìœ¨ ì¶”ì •

### ã€ê³ ë“±í•™êµ - ì§„ë¡œ ì„ íƒ ê³¼ëª©ã€‘
[ë¯¸ì ë¶„II]
- ìˆ˜ì—´ì˜ ê·¹í•œ: ìˆ˜ì—´ì˜ ê·¹í•œ, ê¸‰ìˆ˜
- ì—¬ëŸ¬ ê°€ì§€ ë¯¸ë¶„ë²•: ì—¬ëŸ¬ ê°€ì§€ í•¨ìˆ˜ì˜ ë¯¸ë¶„, í•©ì„±í•¨ìˆ˜/ë§¤ê°œë³€ìˆ˜/ìŒí•¨ìˆ˜ ë¯¸ë¶„
- ì—¬ëŸ¬ ê°€ì§€ ì ë¶„ë²•: ì¹˜í™˜ì ë¶„, ë¶€ë¶„ì ë¶„, ì •ì ë¶„ì˜ í™œìš©

[ê¸°í•˜]
- ì´ì°¨ê³¡ì„ : í¬ë¬¼ì„ , íƒ€ì›, ìŒê³¡ì„ 
- í‰ë©´ë²¡í„°: ë²¡í„°ì˜ ì—°ì‚°, í‰ë©´ë²¡í„°ì˜ ì„±ë¶„ê³¼ ë‚´ì 
- ê³µê°„ë„í˜•ê³¼ ê³µê°„ì¢Œí‘œ: ê³µê°„ë„í˜•, ê³µê°„ì¢Œí‘œ

## ê·œì¹™ (ì—„ê²© ì¤€ìˆ˜)

1. ëª¨ë“  í…ìŠ¤íŠ¸(topic, ai_comment)ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
2. difficulty: concept(ê°œë…), pattern(ìœ í˜•), reasoning(ì‹¬í™”), creative(ìµœìƒìœ„) ì¤‘ í•˜ë‚˜ (4ë‹¨ê³„ ì‹œìŠ¤í…œ)
   - concept: ê¸°ë³¸ ê°œë… í™•ì¸, ê³µì‹ ì§ì ‘ ëŒ€ì…, 1-2ë‹¨ê³„ í•´ê²°
   - pattern: êµê³¼ì„œ ì—°ìŠµë¬¸ì œ ìˆ˜ì¤€, 2-4ë‹¨ê³„ í’€ì´
   - reasoning: 2ê°œ ëŒ€ë‹¨ì› ë³µí•©, 5-6ë‹¨ê³„ í’€ì´, ë…¼ë¦¬ì  ì¶”ë¡ 
   - creative: 3ê°œ ì´ìƒ ëŒ€ë‹¨ì› ë³µí•©, 7ë‹¨ê³„ ì´ìƒ í’€ì´, ì°½ì˜ì  í†µì°°
3. question_type: calculation(ê³„ì‚°), geometry(ë„í˜•), application(ì‘ìš©), proof(ì¦ëª…), graph(ê·¸ë˜í”„), statistics(í†µê³„) ì¤‘ í•˜ë‚˜
4. points: ìˆ«ì (ì†Œìˆ˜ì  í—ˆìš©)
5. ì„œë‹µí˜•ì€ "ì„œë‹µí˜• 1", "ì„œë‹µí˜• 2" í˜•ì‹
6. difficulty_reason: ë‚œì´ë„ íŒë‹¨ ê·¼ê±° (15ì ì´ë‚´)

âš ï¸ ì¤‘ìš” - ì†Œë¬¸ì œ ì²˜ë¦¬:
- (1), (2), (3) ë˜ëŠ” (ê°€), (ë‚˜), (ë‹¤)ê°€ ìˆìœ¼ë©´ í•˜ë‚˜ì˜ ë¬¸ì œë¡œ ì·¨ê¸‰
- ë°°ì ì€ í•©ì‚°
- ë‚œì´ë„ëŠ” ê°€ì¥ ì–´ë ¤ìš´ ì†Œë¬¸ì œ ê¸°ì¤€

7. topic í˜•ì‹: "ê³¼ëª©ëª… > ëŒ€ë‹¨ì› > ì†Œë‹¨ì›"
8. ai_comment: ì •í™•íˆ 2ë¬¸ì¥, ì´ 50ì ì´ë‚´
9. confidence: í•´ë‹¹ ë¬¸í•­ ë¶„ì„ì˜ í™•ì‹ ë„ (0.0 ~ 1.0)
"""

    def _get_unified_prompt(self) -> str:
        """í†µí•© í”„ë¡¬í”„íŠ¸ (ë¶„ë¥˜ + ë¶„ì„ ë™ì‹œ ìˆ˜í–‰) - ì†ë„ ìµœì í™”"""
        return """
ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™êµ ìˆ˜í•™ ì‹œí—˜ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## STEP 0: ì‹œí—˜ì§€ ìœ í˜• íŒë³„ (âš ï¸ ê°€ì¥ ë¨¼ì € ìˆ˜í–‰ - ë§¤ìš° ì¤‘ìš”!)

ì´ë¯¸ì§€ë¥¼ **ì„¸ì‹¬í•˜ê²Œ** ì‚´í´ë³´ê³  ë‹¤ìŒì„ íŒë‹¨í•˜ì„¸ìš”:

### 1. **paper_type** (ì‹œí—˜ì§€ ìœ í˜•) - í•µì‹¬ íŒì • ê¸°ì¤€

ğŸ” **ë¬¸í•­ ë²ˆí˜¸ vs ë³´ê¸° ë²ˆí˜¸ êµ¬ë¶„!**

```
ë¬¸í•­ ë²ˆí˜¸: 1. 2. 3. (ë¬¸ì œ ì¢Œìƒë‹¨, ì•„ë¼ë¹„ì•„ ìˆ«ì)
ë³´ê¸° ë²ˆí˜¸: â‘  â‘¡ â‘¢ â‘£ â‘¤ (ê°ê´€ì‹ ì„ íƒì§€, ì›ë¬¸ì)
```

**"answered"ë¡œ íŒì • (í•˜ë‚˜ë¼ë„ ë³´ì´ë©´ answered!):**
- âœ… **ë¬¸í•­ ë²ˆí˜¸(1. 2. 3.)ì— ë™ê·¸ë¼ë¯¸** â†’ ì •ë‹µ í‘œì‹œ = ì±„ì ë¨!
- âŒ **ë¬¸í•­ ë²ˆí˜¸ì— Xí‘œì‹œ, ë¹—ê¸ˆ(/), ì‚¬ì„ ** â†’ ì˜¤ë‹µ í‘œì‹œ = ì±„ì ë¨!
- ğŸ”´ **O, â—‹, âœ“ í‘œì‹œ** â†’ ì •ë‹µ í‘œì‹œ = ì±„ì ë¨!
- ğŸ”´ **ì ìˆ˜ ê¸°ì¬** (3ì , 0ì , 5/9 ë“±) â†’ ì±„ì ë¨!
- ğŸ“ ë³´ê¸° ë²ˆí˜¸(â‘ â‘¡â‘¢â‘£â‘¤)ì— ì²´í¬/ë™ê·¸ë¼ë¯¸ â†’ í•™ìƒ ë‹µì•ˆ = answered!
- ğŸ“ ì„œìˆ í˜•ì— ì†ê¸€ì”¨ í’€ì´
- ğŸ“ ê³„ì‚° í”ì /ë©”ëª¨

**"blank"ë¡œ íŒì •:**
- ë¬¸í•­ ë²ˆí˜¸ì— **ì•„ë¬´ í‘œì‹œë„ ì—†ìŒ**
- ë³´ê¸°ì— ì²´í¬ **ì—†ìŒ**
- ì†ê¸€ì”¨ **ì „í˜€ ì—†ìŒ**

âš ï¸ **í•µì‹¬: ë¬¸í•­ ë²ˆí˜¸(1. 2. 3.)ë‚˜ ë³´ê¸°(â‘ â‘¡â‘¢)ì— í‘œì‹œê°€ ìˆìœ¼ë©´ "answered"!**

### 2. **grading_status** (ì±„ì  ìƒíƒœ)
- "not_graded": O/X í‘œì‹œê°€ **ì „í˜€** ì—†ìŒ (í•™ìƒì´ í’€ê¸°ë§Œ í•¨)
- "partially_graded": ì¼ë¶€ ë¬¸í•­ì—ë§Œ O/X í‘œì‹œ
- "fully_graded": ëŒ€ë¶€ë¶„ ë¬¸í•­ì— O/X í‘œì‹œ ìˆìŒ

## âš ï¸ ì±„ì  í‘œì‹œ ì¸ì‹ (ë§¤ìš° ì¤‘ìš”!) âš ï¸

### ì •ë‹µ í‘œì‹œ â†’ is_correct: true
- í•™ìƒì´ **ì“´ ë‹µì•ˆ ë°”ë¡œ ì˜†**ì— O, â—‹, âœ“, ì²´í¬ í‘œì‹œ
- ê°ê´€ì‹: í•™ìƒì´ ê³ ë¥¸ ë²ˆí˜¸ì— ë™ê·¸ë¼ë¯¸ í‘œì‹œ
- ì ìˆ˜ê°€ ë°°ì  ê·¸ëŒ€ë¡œ ê¸°ì¬ (ì˜ˆ: 3ì ì§œë¦¬ì— "3" ê¸°ì¬)

### ì˜¤ë‹µ í‘œì‹œ â†’ is_correct: false
- í•™ìƒ ë‹µì•ˆì— X, âœ—, ë¹—ê¸ˆ(/), ì‚¬ì„  í‘œì‹œ
- **ë¬¸ì œë²ˆí˜¸ì— Xí‘œì‹œ, ë¹—ê¸ˆ(/), ì‚¬ì„ ** = í‹€ë¦° ë¬¸ì œ í‘œì‹œ â†’ ì˜¤ë‹µ!
- ë¹¨ê°„íœìœ¼ë¡œ **ì •ë‹µì„ ë”°ë¡œ ì¨ì¤€ ê²½ìš°** â†’ í•™ìƒ ë‹µì´ í‹€ë ¸ë‹¤ëŠ” ì˜ë¯¸
- ì ìˆ˜ê°€ 0 ë˜ëŠ” ê°ì ëœ ê²½ìš°

### ë¯¸ì±„ì  â†’ is_correct: null
- O/X í‘œì‹œê°€ **ì „í˜€ ì—†ëŠ”** ë¬¸í•­
- í•™ìƒì´ ë‹µì„ ì¼ì§€ë§Œ ì±„ì  í‘œì‹œê°€ ì—†ìŒ â†’ **ì ˆëŒ€ ì •ë‹µ ì²˜ë¦¬ ê¸ˆì§€!**
- í™•ì‹ ì´ ì—†ìœ¼ë©´ null ì²˜ë¦¬
- **ì„œìˆ í˜•**: í’€ì´ê°€ ìˆì–´ë„ ì ìˆ˜ ê¸°ì¬ ì—†ìœ¼ë©´ â†’ **null** (ë¯¸ì±„ì !)

### í•µì‹¬ êµ¬ë¶„ë²•
| ìœ„ì¹˜ | í‘œì‹œ | ì˜ë¯¸ |
|------|------|------|
| ë¬¸ì œë²ˆí˜¸(1,2,3) ì˜† ë™ê·¸ë¼ë¯¸ | â‘  â‘¡ â‘¢ | ì •ë‹µ í‘œì‹œ â†’ **ì •ë‹µ** |
| ë¬¸ì œë²ˆí˜¸(1,2,3) ì˜† X/ë¹—ê¸ˆ | âœ— / | í‹€ë¦° ë¬¸ì œ í‘œì‹œ â†’ **ì˜¤ë‹µ** |
| í•™ìƒ ë‹µì•ˆ ì˜† ë™ê·¸ë¼ë¯¸ | ë‹µ: â‘¢ â—‹ | ì •ë‹µ í‘œì‹œ â†’ **ì •ë‹µ** |
| ì•„ë¬´ í‘œì‹œ ì—†ìŒ | ë‹µ: â‘¢ | ë¯¸ì±„ì  â†’ **null** |

## STEP 1: ë¬¸ì œ ì¶”ì¶œ (âš ï¸ ëˆ„ë½ ê¸ˆì§€)

ğŸ¯ **í•µì‹¬ ê·œì¹™: ë°°ì  í‘œì‹œ([Nì ])ê°€ ìˆëŠ” ê³³ = ë¬¸í•­ì´ ìˆëŠ” ê³³**

ì‹œí—˜ì§€ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³ :

**1ë‹¨ê³„: ë°°ì  ì°¾ê¸°**
- [6ì ], [8ì ], [9ì ] ë“±ì˜ ë°°ì  í‘œì‹œë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
- ë°°ì ì´ ìˆëŠ” ê³³ë§ˆë‹¤ ë°˜ë“œì‹œ ë¬¸í•­ì´ ìˆìŠµë‹ˆë‹¤
- ì´ ë¬¸í•­ ìˆ˜ = ë°°ì  í‘œì‹œ ê°œìˆ˜

**2ë‹¨ê³„: ë¬¸í•­ ë²ˆí˜¸ í™•ì¸**
- ë¬¸í•­ ë²ˆí˜¸ê°€ ì—°ì†ì ì¸ì§€ í™•ì¸ (1, 2, 3, 4, ...)
- ì„œë‹µí˜• ë¬¸ì œë„ ë°°ì ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨

âš ï¸ **í•„ìˆ˜**: 1ë²ˆë¶€í„° ë§ˆì§€ë§‰ ë¬¸í•­ê¹Œì§€ **ë¹ ì§ì—†ì´** ëª¨ë‘ ë¶„ì„í•˜ì„¸ìš”.

ğŸ”¢ **ë²ˆí˜¸ ì¶”ë¡  ê·œì¹™**:
- ë²ˆí˜¸ê°€ ê°€ë ¤ì§€ê±°ë‚˜ ì•ˆ ë³´ì—¬ë„, **ìœ„ì¹˜ì™€ ë°°ì ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ì¶”ë¡ **í•˜ì„¸ìš”
- 1ë²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ 2ë²ˆ
- Në²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ N+1ë²ˆ

## STEP 2: ë¬¸í•­ë³„ ë¶„ë¥˜

ê° ë¬¸í•­ì— ëŒ€í•´:
1. í† í”½ ë¶„ë¥˜ (ì–´ë–¤ ê°œë…?)
2. ë‚œì´ë„ íŒì • (concept/pattern/reasoning/creative) - 4ë‹¨ê³„ ì‹œìŠ¤í…œ
3. ë¬¸ì œ ìœ í˜• (calculation/geometry/application/proof/graph/statistics)
4. **í•™ìƒ ë‹µì•ˆì§€ì¸ ê²½ìš°**: is_correct, error_type, earned_points ì¶”ê°€

## JSON ì¶œë ¥ í˜•ì‹

{
    "paper_type": "blank ë˜ëŠ” answered ë˜ëŠ” mixed",
    "paper_type_confidence": 0.95,
    "paper_type_indicators": ["íŒë‹¨ ê·¼ê±°1", "íŒë‹¨ ê·¼ê±°2"],
    "grading_status": "not_graded ë˜ëŠ” partially_graded ë˜ëŠ” fully_graded",
    "grading_indicators": ["ì±„ì  ê·¼ê±°"],
    "requires_human_review": false,
    "review_reason": null,
    "exam_info": {
        "total_questions": 16,
        "total_points": 100,
        "objective_count": 12,
        "subjective_count": 4,
        "earned_total_points": 72,
        "correct_count": 10,
        "wrong_count": 6
    },
    "summary": {
        "difficulty_distribution": {"concept": 0, "pattern": 0, "reasoning": 0, "creative": 0},
        "type_distribution": {
            "calculation": 0, "geometry": 0, "application": 0,
            "proof": 0, "graph": 0, "statistics": 0
        },
        "average_difficulty": "pattern",
        "dominant_type": "calculation"
    },
    "questions": [
        {
            "question_number": 1,
            "difficulty": "concept",
            "question_type": "calculation",
            "points": 3,
            "topic": "ê³µí†µìˆ˜í•™1 > ë‹¤í•­ì‹ > ë‹¤í•­ì‹ì˜ ì—°ì‚°",
            "ai_comment": "í•µì‹¬ ê°œë…. ì£¼ì˜ì‚¬í•­.",
            "difficulty_reason": "ê¸°ë³¸ ê°œë… í™•ì¸",
            "confidence": 0.95,
            "is_correct": true,
            "student_answer": "3",
            "earned_points": 3,
            "error_type": null,
            "grading_rationale": "í•™ìƒ ë‹µ â‘¢ì— Oí‘œì‹œ í™•ì¸"
        },
        {
            "question_number": 2,
            "difficulty": "pattern",
            "question_type": "calculation",
            "points": 4,
            "topic": "ê³µí†µìˆ˜í•™1 > ë°©ì •ì‹ê³¼ ë¶€ë“±ì‹ > ì´ì°¨ë°©ì •ì‹",
            "ai_comment": "ê·¼ì˜ ê³µì‹ í™œìš©. ê³„ì‚° ì£¼ì˜.",
            "difficulty_reason": "ì¼ë°˜ ìœ í˜• ë¬¸ì œ",
            "confidence": 0.60,
            "is_correct": null,
            "student_answer": "5",
            "earned_points": null,
            "error_type": null,
            "grading_rationale": "ì±„ì  í‘œì‹œ ë¶ˆë¶„ëª… - êµì‚¬ í™•ì¸ í•„ìš”",
            "requires_review": true
        },
        {
            "question_number": "ì„œë‹µí˜• 1",
            "difficulty": "reasoning",
            "question_type": "proof",
            "points": 9,
            "topic": "ë¯¸ì ë¶„I > ë¯¸ë¶„ > ë„í•¨ìˆ˜ì˜ í™œìš©",
            "ai_comment": "ì¦ëª… ê³¼ì • ì„œìˆ . ë…¼ë¦¬ì  íë¦„ ì¤‘ìš”.",
            "difficulty_reason": "ë‹¤ë‹¨ê³„ ë…¼ë¦¬ ì „ê°œ",
            "confidence": 0.85,
            "is_correct": false,
            "student_answer": "(í’€ì´ ë‚´ìš©)",
            "earned_points": 5,
            "error_type": "process_error",
            "grading_rationale": "5/9 ë¶€ë¶„ì ìˆ˜ ê¸°ì¬ í™•ì¸, ë…¼ë¦¬ ë¹„ì•½ìœ¼ë¡œ ê°ì ",
            "partial_credit_breakdown": {
                "ê°œë… ì´í•´": {"max": 3, "earned": 3, "note": "ì •í™•í•¨"},
                "í’€ì´ ê³¼ì •": {"max": 4, "earned": 2, "note": "2ë‹¨ê³„ ë…¼ë¦¬ ë¹„ì•½"},
                "ìµœì¢… ë‹µ": {"max": 2, "earned": 0, "note": "ì˜¤ë‹µ"}
            }
        }
    ]
}

## ğŸ” ì±„ì  ê·¼ê±°(grading_rationale) ì‘ì„±ë²•

**ë°˜ë“œì‹œ** ê° ë¬¸í•­ì— íŒì • ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”:
- ì •ë‹µ: "ë‹µ â‘¡ì— ë¹¨ê°„íœ Oí‘œì‹œ", "ë°°ì  4ì  ê·¸ëŒ€ë¡œ ê¸°ì¬"
- ì˜¤ë‹µ: "ë‹µì•ˆì— Xí‘œì‹œ", "0ì  ê¸°ì¬", "ë¬¸ì œë²ˆí˜¸ì— Xí‘œì‹œ/ë¹—ê¸ˆ"
- ë¯¸ì±„ì : "O/X í‘œì‹œ ì—†ìŒ", "ì±„ì  í‘œì‹œ ë¶ˆë¶„ëª…"

## ğŸ“ ì„œìˆ í˜• ë¶€ë¶„ ì ìˆ˜ ê·œì¹™ (partial_credit)

ì„œìˆ í˜• ë¬¸ì œëŠ” ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

| í•­ëª© | ì„¤ëª… | ê°ì  ê¸°ì¤€ |
|------|------|----------|
| ê°œë… ì´í•´ | í•µì‹¬ ê³µì‹/ì •ë¦¬ ì–¸ê¸‰ | ëˆ„ë½ ì‹œ í•´ë‹¹ ì ìˆ˜ 0 |
| í’€ì´ ê³¼ì • | ë…¼ë¦¬ì  ì „ê°œ | ë¹„ì•½/ì˜¤ë¥˜ë‹¹ -1~2ì  |
| ê³„ì‚° ì •í™•ì„± | ìˆ˜ì¹˜ ê³„ì‚° | ë‹¨ìˆœ ì‹¤ìˆ˜ -1ì  |
| ìµœì¢… ë‹µ | ì •ë‹µ ë„ì¶œ | ì˜¤ë‹µ ì‹œ 0ì  |

## âš ï¸ êµì‚¬ ê²€í†  í•„ìš” (requires_human_review)

ë‹¤ìŒ ê²½ìš° `requires_human_review: true` ì„¤ì •:
- confidence < 0.7ì¸ ë¬¸í•­ì´ 2ê°œ ì´ìƒ
- ì±„ì  í‘œì‹œê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ íŒë… ë¶ˆê°€
- ë¶€ë¶„ ì ìˆ˜ íŒì •ì´ ëª¨í˜¸í•œ ì„œìˆ í˜•
- ì´ë¯¸ì§€ í’ˆì§ˆ ë¬¸ì œë¡œ ë‹µì•ˆ ì¸ì‹ ë¶ˆê°€

## í† í”½ ë¶„ë¥˜í‘œ (2022 ê°œì •)

[ê³µí†µìˆ˜í•™1] ë‹¤í•­ì‹, ë°©ì •ì‹ê³¼ ë¶€ë“±ì‹, ê²½ìš°ì˜ ìˆ˜, í–‰ë ¬
[ê³µí†µìˆ˜í•™2] ë„í˜•ì˜ ë°©ì •ì‹, ì§‘í•©ê³¼ ëª…ì œ, í•¨ìˆ˜ì™€ ê·¸ë˜í”„
[ëŒ€ìˆ˜] ì§€ìˆ˜í•¨ìˆ˜ì™€ ë¡œê·¸í•¨ìˆ˜, ì‚¼ê°í•¨ìˆ˜, ìˆ˜ì—´
[ë¯¸ì ë¶„I] í•¨ìˆ˜ì˜ ê·¹í•œê³¼ ì—°ì†, ë¯¸ë¶„, ì ë¶„
[í™•ë¥ ê³¼ í†µê³„] ìˆœì—´ê³¼ ì¡°í•©, í™•ë¥ , í†µê³„
[ë¯¸ì ë¶„II] ìˆ˜ì—´ì˜ ê·¹í•œ, ì—¬ëŸ¬ ê°€ì§€ ë¯¸ë¶„ë²•, ì—¬ëŸ¬ ê°€ì§€ ì ë¶„ë²•
[ê¸°í•˜] ì´ì°¨ê³¡ì„ , í‰ë©´ë²¡í„°, ê³µê°„ë„í˜•ê³¼ ê³µê°„ì¢Œí‘œ

## ì˜¤ë¥˜ ìœ í˜• (error_type) - ì˜¤ë‹µì¼ ë•Œë§Œ í•´ë‹¹

- calculation_error: ê³„ì‚° ì‹¤ìˆ˜
- concept_error: ê°œë… ì˜¤í•´
- careless_mistake: ë‹¨ìˆœ ì‹¤ìˆ˜
- process_error: í’€ì´ ê³¼ì • ì˜¤ë¥˜
- incomplete: ë¯¸ì™„ì„±

## ê·œì¹™ (ì—„ê²© ì¤€ìˆ˜)

1. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
2. **ë¹ˆ ì‹œí—˜ì§€(blank)**: is_correct, student_answer, earned_points, error_type í•„ë“œ ìƒëµ
3. **í•™ìƒ ë‹µì•ˆì§€(answered/mixed)**: ì •ì˜¤ë‹µ í•„ë“œ í¬í•¨
4. **ì±„ì  í‘œì‹œ ì—†ìœ¼ë©´ is_correct: null** (ì •ë‹µ ì¶”ì¸¡ ê¸ˆì§€!)
5. topic í˜•ì‹: "ê³¼ëª©ëª… > ëŒ€ë‹¨ì› > ì†Œë‹¨ì›"
6. ai_comment: ì •í™•íˆ 2ë¬¸ì¥, ì´ 50ì ì´ë‚´
7. confidence: 0.0 ~ 1.0
"""

    def _get_questions_only_prompt(self) -> str:
        """ë¬¸í•­ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸ (ì •ì˜¤ë‹µ ë¶„ì„ ì œì™¸) - ë¹ ë¥¸ 1ì°¨ ë¶„ì„ìš©"""
        return """
ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™êµ ìˆ˜í•™ ì‹œí—˜ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ëª©í‘œ: ë¬¸í•­ ë¶„ì„ë§Œ ìˆ˜í–‰ (ì •ì˜¤ë‹µ ë¶„ì„ ì œì™¸)

ì´ ë¶„ì„ì—ì„œëŠ” **ë¬¸ì œ ìì²´ë§Œ ë¶„ì„**í•©ë‹ˆë‹¤.
- í•™ìƒ ë‹µì•ˆ, ì±„ì  í‘œì‹œ, ì •ì˜¤ë‹µì€ ë¶„ì„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë¬¸ì œì˜ ìœ í˜•, ë‚œì´ë„, í† í”½ë§Œ íŒŒì•…í•©ë‹ˆë‹¤.

## STEP 0: ì‹œí—˜ì§€ ìœ í˜• íŒë³„

ì´ë¯¸ì§€ë¥¼ ì‚´í´ë³´ê³  ë‹¤ìŒì„ íŒë‹¨í•˜ì„¸ìš”:

### paper_type (ì‹œí—˜ì§€ ìœ í˜•)
- **"blank"**: ë¹ˆ ì‹œí—˜ì§€ (ë‹µì•ˆ ì‘ì„± í”ì  ì—†ìŒ)
- **"answered"**: í•™ìƒ ë‹µì•ˆì§€ (ë‹µì•ˆ ì‘ì„± í”ì  ìˆìŒ)
- **"mixed"**: ì¼ë¶€ë§Œ ë‹µì•ˆ ì‘ì„±ë¨

âš ï¸ ì´ ë¶„ì„ì—ì„œëŠ” paper_typeë§Œ íŒë‹¨í•˜ê³ , ì •ì˜¤ë‹µì€ ë¶„ì„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## STEP 1: ë¬¸ì œ ì¶”ì¶œ (âš ï¸ ëˆ„ë½ ê¸ˆì§€)

ğŸ¯ **í•µì‹¬ ê·œì¹™: ë°°ì  í‘œì‹œ([Nì ])ê°€ ìˆëŠ” ê³³ = ë¬¸í•­ì´ ìˆëŠ” ê³³**

ì‹œí—˜ì§€ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³ :

**1ë‹¨ê³„: ë°°ì  ì°¾ê¸°**
- [6ì ], [8ì ], [9ì ] ë“±ì˜ ë°°ì  í‘œì‹œë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
- ë°°ì ì´ ìˆëŠ” ê³³ë§ˆë‹¤ ë°˜ë“œì‹œ ë¬¸í•­ì´ ìˆìŠµë‹ˆë‹¤
- ì´ ë¬¸í•­ ìˆ˜ = ë°°ì  í‘œì‹œ ê°œìˆ˜

**2ë‹¨ê³„: ë¬¸í•­ ë²ˆí˜¸ í™•ì¸**
- ë¬¸í•­ ë²ˆí˜¸ê°€ ì—°ì†ì ì¸ì§€ í™•ì¸ (1, 2, 3, 4, ...)
- ì„œë‹µí˜• ë¬¸ì œë„ ë°°ì ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨

âš ï¸ **í•„ìˆ˜**: 1ë²ˆë¶€í„° ë§ˆì§€ë§‰ ë¬¸í•­ê¹Œì§€ **ë¹ ì§ì—†ì´** ëª¨ë‘ ë¶„ì„í•˜ì„¸ìš”.

ğŸ”¢ **ë²ˆí˜¸ ì¶”ë¡  ê·œì¹™**:
- ë²ˆí˜¸ê°€ ê°€ë ¤ì§€ê±°ë‚˜ ì•ˆ ë³´ì—¬ë„, **ìœ„ì¹˜ì™€ ë°°ì ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ì¶”ë¡ **í•˜ì„¸ìš”
- 1ë²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ 2ë²ˆ
- Në²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ N+1ë²ˆ

## STEP 2: ë¬¸í•­ë³„ ë¶„ë¥˜

ê° ë¬¸í•­ì— ëŒ€í•´:
1. í† í”½ ë¶„ë¥˜ (ì–´ë–¤ ê°œë…?)
2. ë‚œì´ë„ íŒì • (concept/pattern/reasoning/creative) - 4ë‹¨ê³„ ì‹œìŠ¤í…œ
3. ë¬¸ì œ ìœ í˜• (calculation/geometry/application/proof/graph/statistics)
4. ë¬¸ì œ í˜•ì‹ (objective: ê°ê´€ì‹, short_answer: ë‹¨ë‹µí˜•, essay: ì„œìˆ í˜•)

## JSON ì¶œë ¥ í˜•ì‹

{
    "paper_type": "blank ë˜ëŠ” answered ë˜ëŠ” mixed",
    "paper_type_confidence": 0.95,
    "paper_type_indicators": ["íŒë‹¨ ê·¼ê±°"],
    "grading_status": "not_analyzed",
    "exam_info": {
        "total_questions": 16,
        "total_points": 100,
        "objective_count": 12,
        "subjective_count": 4
    },
    "summary": {
        "difficulty_distribution": {"concept": 0, "pattern": 0, "reasoning": 0, "creative": 0},
        "type_distribution": {
            "calculation": 0, "geometry": 0, "application": 0,
            "proof": 0, "graph": 0, "statistics": 0
        },
        "average_difficulty": "pattern",
        "dominant_type": "calculation"
    },
    "questions": [
        {
            "question_number": 1,
            "difficulty": "concept",
            "question_type": "calculation",
            "question_format": "objective",
            "points": 3,
            "topic": "ê³µí†µìˆ˜í•™1 > ë‹¤í•­ì‹ > ë‹¤í•­ì‹ì˜ ì—°ì‚°",
            "ai_comment": "í•µì‹¬ ê°œë…. ì£¼ì˜ì‚¬í•­.",
            "difficulty_reason": "ê¸°ë³¸ ê°œë… í™•ì¸",
            "confidence": 0.95
        },
        {
            "question_number": "ì„œë‹µí˜• 1",
            "difficulty": "reasoning",
            "question_type": "proof",
            "question_format": "essay",
            "points": 9,
            "topic": "ë¯¸ì ë¶„I > ë¯¸ë¶„ > ë„í•¨ìˆ˜ì˜ í™œìš©",
            "ai_comment": "ì¦ëª… ê³¼ì • ì„œìˆ . ë…¼ë¦¬ì  íë¦„ ì¤‘ìš”.",
            "difficulty_reason": "ë‹¤ë‹¨ê³„ ë…¼ë¦¬ ì „ê°œ",
            "confidence": 0.85
        }
    ]
}

## í† í”½ ë¶„ë¥˜í‘œ (2022 ê°œì •)

[ê³µí†µìˆ˜í•™1] ë‹¤í•­ì‹, ë°©ì •ì‹ê³¼ ë¶€ë“±ì‹, ê²½ìš°ì˜ ìˆ˜, í–‰ë ¬
[ê³µí†µìˆ˜í•™2] ë„í˜•ì˜ ë°©ì •ì‹, ì§‘í•©ê³¼ ëª…ì œ, í•¨ìˆ˜ì™€ ê·¸ë˜í”„
[ëŒ€ìˆ˜] ì§€ìˆ˜í•¨ìˆ˜ì™€ ë¡œê·¸í•¨ìˆ˜, ì‚¼ê°í•¨ìˆ˜, ìˆ˜ì—´
[ë¯¸ì ë¶„I] í•¨ìˆ˜ì˜ ê·¹í•œê³¼ ì—°ì†, ë¯¸ë¶„, ì ë¶„
[í™•ë¥ ê³¼ í†µê³„] ìˆœì—´ê³¼ ì¡°í•©, í™•ë¥ , í†µê³„
[ë¯¸ì ë¶„II] ìˆ˜ì—´ì˜ ê·¹í•œ, ì—¬ëŸ¬ ê°€ì§€ ë¯¸ë¶„ë²•, ì—¬ëŸ¬ ê°€ì§€ ì ë¶„ë²•
[ê¸°í•˜] ì´ì°¨ê³¡ì„ , í‰ë©´ë²¡í„°, ê³µê°„ë„í˜•ê³¼ ê³µê°„ì¢Œí‘œ

## ê·œì¹™ (ì—„ê²© ì¤€ìˆ˜)

1. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
2. **ì •ì˜¤ë‹µ ê´€ë ¨ í•„ë“œ ì œì™¸**: is_correct, student_answer, earned_points, error_type í•„ë“œë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
3. topic í˜•ì‹: "ê³¼ëª©ëª… > ëŒ€ë‹¨ì› > ì†Œë‹¨ì›"
4. ai_comment: ì •í™•íˆ 2ë¬¸ì¥, ì´ 50ì ì´ë‚´
5. confidence: 0.0 ~ 1.0 (ë¬¸ì œ ì¸ì‹ ì‹ ë¢°ë„)
"""

    def _get_student_prompt(self) -> str:
        """í•™ìƒ ë‹µì•ˆì§€ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return """
ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™êµ ìˆ˜í•™ ì‹œí—˜ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ê²ƒì€ **í•™ìƒì´ í‘¼ ì‹œí—˜ì§€**ì…ë‹ˆë‹¤. ì •ì˜¤ë‹µ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.

## ë¶„ì„ ë‹¨ê³„ (Chain of Thought)

### STEP 1: ë¬¸ì œ ë° ì±„ì  ì¶”ì¶œ (âš ï¸ ëˆ„ë½ ê¸ˆì§€)

ğŸ¯ **í•µì‹¬ ê·œì¹™: ë°°ì  í‘œì‹œ([Nì ])ê°€ ìˆëŠ” ê³³ = ë¬¸í•­ì´ ìˆëŠ” ê³³**

ì‹œí—˜ì§€ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³  ë‹¤ìŒì„ íŒŒì•…í•˜ì„¸ìš”:

**1ë‹¨ê³„: ë°°ì  ì°¾ê¸°**
- [6ì ], [8ì ], [9ì ] ë“±ì˜ ë°°ì  í‘œì‹œë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
- ë°°ì ì´ ìˆëŠ” ê³³ë§ˆë‹¤ ë°˜ë“œì‹œ ë¬¸í•­ì´ ìˆìŠµë‹ˆë‹¤
- ì´ ë¬¸í•­ ìˆ˜ = ë°°ì  í‘œì‹œ ê°œìˆ˜

**2ë‹¨ê³„: ì±„ì  ì •ë³´ ìˆ˜ì§‘**
- **ì •ë‹µ/ì˜¤ë‹µ í‘œì‹œ ì¸ì‹** (O, X, âœ“, âœ—, ë¹¨ê°„íœ, ë™ê·¸ë¼ë¯¸ ë“±)
- **í•™ìƒì´ ì‘ì„±í•œ ë‹µì•ˆ** (ì„ íƒì§€ ë²ˆí˜¸, ì„œìˆ  ë‚´ìš© ë“±)
- **íšë“ ì ìˆ˜** (ë¶€ë¶„ ì ìˆ˜ í¬í•¨)

âš ï¸ **í•„ìˆ˜**: 1ë²ˆë¶€í„° ë§ˆì§€ë§‰ ë¬¸í•­ê¹Œì§€ **ë¹ ì§ì—†ì´** ëª¨ë‘ ë¶„ì„í•˜ì„¸ìš”.

### ğŸ”´ ì„œë‹µí˜•(ì„œìˆ í˜•) ë¬¸ì œ ì¸ì‹ (ë§¤ìš° ì¤‘ìš”!)
- "ì„œë‹µí˜•", "ì„œìˆ í˜•", "ì£¼ê´€ì‹" í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¬¸ì œ ë°˜ë“œì‹œ í¬í•¨
- "ì„œë‹µí˜• 1", "ì„œë‹µí˜• 2", "[ì„œë‹µí˜• 1]" ë“±ì˜ í˜•ì‹ ì¸ì‹
- ë²ˆí˜¸ í˜•ì‹: "ì„œë‹µí˜• 1" â†’ question_number: "ì„œë‹µí˜• 1"
- ì„œë‹µí˜•ë„ ë°°ì ì´ ìˆìœ¼ë©´ questions ë°°ì—´ì— í¬í•¨ í•„ìˆ˜!
- ì„œë‹µí˜• ì±„ì : ì ìˆ˜ ê¸°ì¬ í™•ì¸ (ì˜ˆ: 5/9ì  â†’ ë¶€ë¶„ì ìˆ˜)
- ì±„ì  í‘œì‹œ(X, O, âœ“)ê°€ í¬ê²Œ í‘œì‹œë˜ì–´ ìˆì–´ë„ í•´ë‹¹ ë¬¸í•­ì„ ë°˜ë“œì‹œ í¬í•¨
- ì†ê¸€ì”¨, ë¹¨ê°„íœ í‘œì‹œê°€ ë§ì•„ë„ **ë°°ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸í•­ ì¸ì‹**
- í‹€ë¦° ë¬¸ì œë„ ê±´ë„ˆë›°ì§€ ë§ê³  ë°˜ë“œì‹œ ë¶„ì„ì— í¬í•¨

ğŸ”¢ **ë²ˆí˜¸ ì¶”ë¡  ê·œì¹™**:
- ë²ˆí˜¸ê°€ ê°€ë ¤ì§€ê±°ë‚˜ ì•ˆ ë³´ì—¬ë„, **ìœ„ì¹˜ì™€ ë°°ì ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ì¶”ë¡ **í•˜ì„¸ìš”
- 1ë²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ 2ë²ˆ
- Në²ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ N+1ë²ˆ
- í° X í‘œì‹œë‚˜ ì±„ì  ë§ˆí¬ë¡œ ë²ˆí˜¸ê°€ ê°€ë ¤ì ¸ë„ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ ë¶€ì—¬

### STEP 2: ë¬¸í•­ë³„ ë¶„ë¥˜ + ì •ì˜¤ë‹µ ë¶„ì„
ê° ë¬¸í•­ì— ëŒ€í•´:
1. ì–´ë–¤ ê°œë…ì„ ë¬»ëŠ”ê°€? â†’ í† í”½ ë¶„ë¥˜
2. ì–¼ë§ˆë‚˜ ì–´ë ¤ìš´ê°€? â†’ ë‚œì´ë„ íŒì • (concept/pattern/reasoning/creative)
3. ì–´ë–¤ ìœ í˜•ì¸ê°€? â†’ ë¬¸ì œ ìœ í˜•
4. **ì •ë‹µì¸ê°€ ì˜¤ë‹µì¸ê°€?** â†’ is_correct
5. **ì˜¤ë‹µì¼ ê²½ìš° ì˜¤ë¥˜ ìœ í˜•** â†’ error_type

### STEP 3: JSON ì¶œë ¥

{
    "requires_human_review": false,
    "review_reason": null,
    "exam_info": {
        "total_questions": 16,
        "total_points": 100,
        "objective_count": 12,
        "subjective_count": 4,
        "earned_total_points": 72,
        "correct_count": 10,
        "wrong_count": 6
    },
    "summary": {
        "difficulty_distribution": {"concept": 0, "pattern": 0, "reasoning": 0, "creative": 0},
        "type_distribution": {
            "calculation": 0, "geometry": 0, "application": 0,
            "proof": 0, "graph": 0, "statistics": 0
        },
        "average_difficulty": "pattern",
        "dominant_type": "calculation"
    },
    "questions": [
        {
            "question_number": 1,
            "difficulty": "concept",
            "question_type": "calculation",
            "points": 3,
            "topic": "ê³µí†µìˆ˜í•™1 > ë‹¤í•­ì‹ > ë‹¤í•­ì‹ì˜ ì—°ì‚°",
            "ai_comment": "í•µì‹¬ ê°œë…. ì£¼ì˜ì‚¬í•­.",
            "difficulty_reason": "ê¸°ë³¸ ê°œë… í™•ì¸",
            "confidence": 0.95,
            "is_correct": true,
            "student_answer": "â‘¢",
            "earned_points": 3,
            "error_type": null,
            "grading_rationale": "ë‹µ â‘¢ì— ë¹¨ê°„íœ Oí‘œì‹œ í™•ì¸"
        },
        {
            "question_number": 2,
            "difficulty": "pattern",
            "question_type": "calculation",
            "points": 4,
            "topic": "ê³µí†µìˆ˜í•™1 > ë°©ì •ì‹ê³¼ ë¶€ë“±ì‹ > ì´ì°¨ë°©ì •ì‹",
            "ai_comment": "ê·¼ì˜ ê³µì‹ í™œìš©. íŒë³„ì‹ ì£¼ì˜.",
            "difficulty_reason": "ì¼ë°˜ ìœ í˜• ë¬¸ì œ",
            "confidence": 0.90,
            "is_correct": false,
            "student_answer": "â‘¡",
            "earned_points": 0,
            "error_type": "calculation_error",
            "grading_rationale": "ë‹µì•ˆì— Xí‘œì‹œ, ë¹¨ê°„íœìœ¼ë¡œ ì •ë‹µ â‘£ ê¸°ì¬ë¨"
        },
        {
            "question_number": "ì„œë‹µí˜• 1",
            "difficulty": "reasoning",
            "question_type": "proof",
            "points": 9,
            "topic": "ë¯¸ì ë¶„I > ë¯¸ë¶„ > ë„í•¨ìˆ˜ì˜ í™œìš©",
            "ai_comment": "ì¦ëª… ê³¼ì • ì„œìˆ . ë…¼ë¦¬ì  íë¦„ ì¤‘ìš”.",
            "difficulty_reason": "ë‹¤ë‹¨ê³„ ë…¼ë¦¬ ì „ê°œ",
            "confidence": 0.85,
            "is_correct": false,
            "student_answer": "(í’€ì´ ë‚´ìš©)",
            "earned_points": 5,
            "error_type": "process_error",
            "grading_rationale": "5/9 ë¶€ë¶„ì ìˆ˜ ê¸°ì¬ í™•ì¸",
            "partial_credit_breakdown": {
                "ê°œë… ì´í•´": {"max": 3, "earned": 3, "note": "ì •í™•í•¨"},
                "í’€ì´ ê³¼ì •": {"max": 4, "earned": 2, "note": "2ë‹¨ê³„ ë…¼ë¦¬ ë¹„ì•½"},
                "ìµœì¢… ë‹µ": {"max": 2, "earned": 0, "note": "ì˜¤ë‹µ"}
            }
        }
    ]
}

## ğŸ” ì±„ì  ê·¼ê±°(grading_rationale) - í•„ìˆ˜ ì‘ì„±!

**ëª¨ë“  ë¬¸í•­ì— íŒì • ê·¼ê±°ë¥¼ ëª…ì‹œ**:
- âœ… ì •ë‹µ: "ë‹µ â‘¡ì— Oí‘œì‹œ", "ë°°ì  4ì  ê·¸ëŒ€ë¡œ ê¸°ì¬"
- âŒ ì˜¤ë‹µ: "ë‹µì•ˆì— Xí‘œì‹œ", "0ì  ê¸°ì¬", "ë¬¸ì œë²ˆí˜¸ì— X/ë¹—ê¸ˆ"
- â“ ë¯¸ì±„ì : "O/X í‘œì‹œ ì—†ìŒ", "ì±„ì  í‘œì‹œ ë¶ˆë¶„ëª…"

## ğŸ“ ì„œìˆ í˜• ë¶€ë¶„ ì ìˆ˜ ë¶„ì„ (partial_credit_breakdown)

ì„œìˆ í˜• ë¬¸ì œëŠ” ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜ ë¶„ì„:
- ê°œë… ì´í•´: í•µì‹¬ ê³µì‹/ì •ë¦¬ ì–¸ê¸‰ ì—¬ë¶€
- í’€ì´ ê³¼ì •: ë…¼ë¦¬ì  ì „ê°œ (ë¹„ì•½/ì˜¤ë¥˜ë‹¹ ê°ì )
- ê³„ì‚° ì •í™•ì„±: ìˆ˜ì¹˜ ê³„ì‚° ì •í™•ë„
- ìµœì¢… ë‹µ: ì •ë‹µ ë„ì¶œ ì—¬ë¶€

## âš ï¸ êµì‚¬ ê²€í†  í•„ìš” (requires_human_review: true)

ë‹¤ìŒ ê²½ìš° ì„¤ì •:
- confidence < 0.7ì¸ ë¬¸í•­ì´ 2ê°œ ì´ìƒ
- ì±„ì  í‘œì‹œ ë¶ˆë¶„ëª…/íŒë… ë¶ˆê°€
- ë¶€ë¶„ ì ìˆ˜ íŒì •ì´ ëª¨í˜¸í•œ ì„œìˆ í˜•

## ì˜¤ë¥˜ ìœ í˜• (error_type)

- calculation_error: ê³„ì‚° ì‹¤ìˆ˜ (ë¶€í˜¸, ì‚¬ì¹™ì—°ì‚° ë“±)
- concept_error: ê°œë… ì˜¤í•´ (ê³µì‹, ì •ì˜ ë“±)
- careless_mistake: ë‹¨ìˆœ ì‹¤ìˆ˜ (ë¬¸ì œ ì˜ëª» ì½ìŒ, ë‹µì•ˆ ì˜ëª» ê¸°ì¬)
- process_error: í’€ì´ ê³¼ì • ì˜¤ë¥˜ (ë…¼ë¦¬ì  ë¹„ì•½)
- incomplete: ë¯¸ì™„ì„± (ì‹œê°„ ë¶€ì¡±, í¬ê¸°)

## âš ï¸ ì±„ì  í‘œì‹œ ì¸ì‹ (ê°€ì¥ ì¤‘ìš”!) âš ï¸

### ì±„ì  íŒì • í…Œì´ë¸” (ë°˜ë“œì‹œ ì°¸ì¡°!)

| ìƒí™© | is_correct | íŒë‹¨ ê·¼ê±° |
|------|------------|-----------|
| í•™ìƒ ë‹µì•ˆ ì˜† O/âœ“ í‘œì‹œ | true | ì •ë‹µ í‘œì‹œ |
| ë°°ì  ê·¸ëŒ€ë¡œ ì ìˆ˜ ê¸°ì¬ | true | 3ì â†’"3" |
| í•™ìƒ ë‹µì•ˆì— X/ë¹—ê¸ˆ í‘œì‹œ | false | ì˜¤ë‹µ í‘œì‹œ |
| ë™ê·¸ë¼ë¯¸ + X/ë¹—ê¸ˆ/0ì  | false | í™•ì‹¤í•œ ì˜¤ë‹µ |
| ë¹¨ê°„íœìœ¼ë¡œ ì •ë‹µ ë”°ë¡œ ê¸°ì¬ | false | í•™ìƒ ë‹µì´ í‹€ë¦¼ |
| ì ìˆ˜ 0ì  ë˜ëŠ” ê°ì  | false | ì˜¤ë‹µ |
| **ë™ê·¸ë¼ë¯¸ë§Œ ìˆìŒ (ë‹¤ë¥¸ í‘œì‹œ ì—†ìŒ)** | **null** | í™•ì¸ ë¶ˆê°€ |
| **O/X í‘œì‹œ ì „í˜€ ì—†ìŒ** | **null** | ë¯¸ì±„ì ! |
| ë‹µì€ ì¼ì§€ë§Œ í‘œì‹œ ì—†ìŒ | null | ë¯¸ì±„ì ! |

### âš ï¸ ë™ê·¸ë¼ë¯¸ íŒë‹¨ ì‹œ ì£¼ì˜ (ë§¤ìš° ì¤‘ìš”!)
- ë™ê·¸ë¼ë¯¸ë§Œ ë‹¨ë…ìœ¼ë¡œ ìˆìœ¼ë©´ â†’ is_correct: **null** (í™•ì¸ ë¶ˆê°€)
- ë™ê·¸ë¼ë¯¸ + Xí‘œì‹œ/ë¹—ê¸ˆ/0ì  â†’ is_correct: false (í™•ì‹¤í•œ ì˜¤ë‹µ)
- ë™ê·¸ë¼ë¯¸ + ë¹¨ê°„íœ ì •ë‹µ â†’ is_correct: false (í•™ìƒì´ í‹€ë¦¼)
- **ì¶”ê°€ ì¦ê±° ì—†ì´ ë™ê·¸ë¼ë¯¸ë§Œìœ¼ë¡œ ì˜¤ë‹µ íŒì • ê¸ˆì§€!**

### í•µì‹¬ êµ¬ë¶„ë²• (í˜¼ë™ ì£¼ì˜!)
```
âŒ í™•ì‹¤íˆ í‹€ë¦° ê²ƒ: â‘ X (ë™ê·¸ë¼ë¯¸+Xí‘œì‹œ) â†’ is_correct: false
âœ… í™•ì‹¤íˆ ë§ëŠ” ê²ƒ: ë‹µ: â‘¢ â—‹ (ë‹µì•ˆì— Oí‘œì‹œ) â†’ is_correct: true
â“ ë¶ˆí™•ì‹¤: â‘  (ë™ê·¸ë¼ë¯¸ë§Œ) â†’ is_correct: null (ì¶”ê°€ ì¦ê±° í•„ìš”!)
â“ ë¯¸ì±„ì : ë‹µ: â‘¢ (í‘œì‹œ ì—†ìŒ) â†’ is_correct: null
```

### ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­
- ì±„ì  í‘œì‹œ ì—†ì´ **ì •ë‹µìœ¼ë¡œ ì¶”ì¸¡ ê¸ˆì§€**
- **ë™ê·¸ë¼ë¯¸ë§Œìœ¼ë¡œ ì˜¤ë‹µ íŒì • ê¸ˆì§€** (Xí‘œì‹œ, 0ì  ë“± ì¶”ê°€ ì¦ê±° í•„ìš”!)
- í•™ìƒì´ ë‹µì„ ì¼ë‹¤ê³  ì •ë‹µ ì²˜ë¦¬ ê¸ˆì§€ (í‘œì‹œ í™•ì¸ í•„ìˆ˜!)
- ë¶ˆí™•ì‹¤í•˜ë©´ is_correct: null ì²˜ë¦¬

### âš ï¸ ì„œìˆ í˜•/ì£¼ê´€ì‹ ë¬¸ì œ ì±„ì  (íŠ¹ë³„ ì£¼ì˜!)
| ìƒí™© | is_correct | íŒë‹¨ ê·¼ê±° |
|------|------------|-----------|
| ì ìˆ˜ ê¸°ì¬ (9/9, 10ì  ë“±) | true | ë§Œì  íšë“ |
| ë¶€ë¶„ ì ìˆ˜ (5/9 ë“±) | false | ê°ì ë¨ |
| 0ì  ë˜ëŠ” Xí‘œì‹œ | false | ì˜¤ë‹µ |
| **í’€ì´ë§Œ ìˆê³  ì ìˆ˜ ì—†ìŒ** | **null** | **ë¯¸ì±„ì !** |
| ë¹ˆì¹¸/ë¯¸ì‘ì„± | null | ë¯¸ë‹µ |

**í•µì‹¬**: ì„œìˆ í˜•ì€ ë°˜ë“œì‹œ **ì ìˆ˜ ê¸°ì¬ í™•ì¸ í›„** íŒì •!
- í•™ìƒì´ í’€ì´ë¥¼ ê¸¸ê²Œ ì‘ì„±í–ˆì–´ë„ ì ìˆ˜ê°€ ì—†ìœ¼ë©´ â†’ is_correct: null
- í’€ì´ê°€ ë§ì•„ ë³´ì—¬ë„ ì±„ì  ì ìˆ˜ ì—†ìœ¼ë©´ â†’ is_correct: null

## ê·œì¹™ (ì—„ê²© ì¤€ìˆ˜)

1. ëª¨ë“  í…ìŠ¤íŠ¸(topic, ai_comment)ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
2. difficulty: concept(ê°œë…), pattern(ìœ í˜•), reasoning(ì‹¬í™”), creative(ìµœìƒìœ„) ì¤‘ í•˜ë‚˜ (4ë‹¨ê³„ ì‹œìŠ¤í…œ)
3. question_type: calculation, geometry, application, proof, graph, statistics ì¤‘ í•˜ë‚˜
4. points: ìˆ«ì
5. **ì±„ì  í‘œì‹œ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ is_correct: null** (ì¶”ì¸¡ ê¸ˆì§€!)
6. topic í˜•ì‹: "ê³¼ëª©ëª… > ëŒ€ë‹¨ì› > ì†Œë‹¨ì›"
7. ai_comment: ì •í™•íˆ 2ë¬¸ì¥, ì´ 50ì ì´ë‚´
8. confidence: í•´ë‹¹ ë¬¸í•­ ë¶„ì„ì˜ í™•ì‹ ë„ (0.0 ~ 1.0)
9. difficulty_reason: ë‚œì´ë„ íŒë‹¨ ê·¼ê±° (15ì ì´ë‚´)
"""


ai_engine = AIEngine()
